{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laney048/Startup/blob/main/Startup_Proposal_Evaluation_Script_(Refactored_Prompts)3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"Startup Proposal Evaluation Script (Enhanced with Temperature and Evaluation Method Controls)\n",
        "\n",
        "This script evaluates startup proposals using three main criteria: Feasibility, Potential for Impact, and Innovative Approach.\n",
        "All evaluations are performed by the OpenAI API.\n",
        "\n",
        "Key Features:\n",
        "- Evaluates proposals based on user-defined criteria and scoring scale.\n",
        "- Supports different prompt forms (\"default\", \"no_harsh\", \"no_rubric\", \"no_examples\", \"without_everything\") for varied instructions and tones.\n",
        "- All supported forms use a single API call to get all three criteria evaluations in one JSON object.\n",
        "- Includes dynamic few-shot examples (optional) in the prompt.\n",
        "- Calculates an overall weighted score based on user-defined percentages.\n",
        "- Generates heatmaps to visualize proposal scores.\n",
        "- Processes one selected prompt form at a time, generating specific outputs for that form.\n",
        "\n",
        "ENHANCED FEATURES:\n",
        "1. Temperature Control: Configurable temperature values (0.1, 0.3, 0.5, 0.7, 1.0) for controlled experiments\n",
        "2. Evaluation Method Control:\n",
        "   - 'simultaneous': Evaluate all 3 criteria in one API call (original method, potential halo effect)\n",
        "   - 'sequential': Evaluate each criterion separately in 3 API calls (reduced halo effect)\n",
        "\n",
        "Prompt Content Philosophy:\n",
        "- Each prompt form has its own independent system message, instruction, challenge description, and criteria definitions.\n",
        "- 'SCORING_SCALE_TEXT' and 'JSON_OUTPUT_FORMAT_PROMPT_SNIPPET' remain universal where applicable.\n",
        "- Example inclusion is finely controlled by `include_static_examples_rationales` and `use_dynamic_few_shot_examples` flags.\n",
        "\"\"\"\n",
        "\n",
        "# --- 1. Imports and Setup ---\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import numpy as np # Import numpy for NaN\n",
        "# google.colab.files is used for downloading files in Google Colab environment\n",
        "from google.colab import files\n",
        "# tqdm is used for displaying progress bars\n",
        "from tqdm import tqdm\n",
        "# OpenAI library for interacting with GPT models\n",
        "from openai import OpenAI\n",
        "from openai import RateLimitError, APIConnectionError, APIStatusError # Import specific API error types\n",
        "# Matplotlib and Seaborn for plotting visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Requests is a common library for making HTTP requests (general purpose, not specifically used for API calls here as openai client handles it)\n",
        "import requests\n",
        "# concurrent.futures for parallel execution of API calls\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "tqdm.pandas() # Enable tqdm progress bar for pandas operations"
      ],
      "metadata": {
        "id": "_UkfENaWgpxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#User Configuration settings"
      ],
      "metadata": {
        "id": "TgGC8jyCwhP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Modify the following two variables to run different experiments:\n",
        "1. EXPERIMENT_TEMPERATURE: Controlling the randomness of AI responses\n",
        "2. EXPERIMENT_EVALUATION_METHOD: Control assessment method\n",
        "'''\n",
        "# Available temperature values for experiments\n",
        "TEMPERATURE_VALUES = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
        "\n",
        "# Available evaluation methods\n",
        "EVALUATION_METHODS = {\n",
        "    'simultaneous': 'Evaluate all 3 criteria in one API call (faster, potential halo effect)',\n",
        "    'sequential': 'Evaluate each criterion separately in 3 calls (slower, reduces halo effect)'\n",
        "}\n",
        "\n",
        "# ===== USER CONFIGURATION SETTINGS =====\n",
        "# Modify these two values to run different experiments\n",
        "EXPERIMENT_TEMPERATURE = 1 # Choose from TEMPERATURE_VALUES: 0.1, 0.3, 0.5, 0.7, or 1.0\n",
        "EXPERIMENT_EVALUATION_METHOD = 'sequential'  # Choose: 'simultaneous' or 'sequential'\n",
        "\n",
        "\n",
        "def validate_experiment_config():\n",
        "    \"\"\"Validate configuration settings\"\"\"\n",
        "    if EXPERIMENT_TEMPERATURE not in TEMPERATURE_VALUES:\n",
        "        raise ValueError(f\"Temperature must be one of {TEMPERATURE_VALUES}. Got: {EXPERIMENT_TEMPERATURE}\")\n",
        "    if EXPERIMENT_EVALUATION_METHOD not in EVALUATION_METHODS:\n",
        "        raise ValueError(f\"Evaluation method must be one of {list(EVALUATION_METHODS.keys())}. Got: {EXPERIMENT_EVALUATION_METHOD}\")\n",
        "    return True\n",
        "\n",
        "def get_experiment_config():\n",
        "    \"\"\"Return current experimental configuration as a dictionary\"\"\"\n",
        "    return {\n",
        "        'temperature': EXPERIMENT_TEMPERATURE,\n",
        "        'evaluation_method': EXPERIMENT_EVALUATION_METHOD,\n",
        "        'method_description': EVALUATION_METHODS.get(EXPERIMENT_EVALUATION_METHOD, 'Unknown')\n",
        "    }\n",
        "\n",
        "def print_experiment_config():\n",
        "    \"\"\"Print current configuration in a formatted way\"\"\"\n",
        "\n",
        "    print(\"EXPERIMENT CONFIGURATION\")\n",
        "\n",
        "    config = get_experiment_config()\n",
        "    print(f\"Temperature:        {config['temperature']}\")\n",
        "    print(f\"Evaluation Method:  {config['evaluation_method']}\")\n",
        "    print(f\"Description:        {config['method_description']}\")\n",
        "    print()\n",
        "\n",
        "# Validate configuration on module load\n",
        "try:\n",
        "    validate_experiment_config()\n",
        "    print(\"Experiment configuration validated successfully\")\n",
        "    print_experiment_config()\n",
        "except ValueError as e:\n",
        "    print(f\"Configuration Error: {e}\")\n",
        "    print(\"Please fix the EXPERIMENT_TEMPERATURE and EXPERIMENT_EVALUATION_METHOD variables\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "i353vQ0qsV9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Universal Prompt Components"
      ],
      "metadata": {
        "id": "UsyOoiHcgyk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These components are extracted into independent variables to avoid duplication in PROMPT_CONTENT_CONFIG\n",
        "\n",
        "# Universal Scoring Scale - Applies to all prompt forms that use a scale\n",
        "SCORING_SCALE_TEXT = \"\"\"\n",
        "Passing Probability Scale (use these exact values or scores between them):\n",
        "- 1: Strongly Disagree - The submission clearly fails on this dimension.\n",
        "- 2: Somewhat Disagree - Significant weaknesses outweigh strengths.\n",
        "- 3: Neither Agree nor Disagree - Mixed evidence; meets only minimum expectations.\n",
        "- 4: Somewhat Agree - Strong overall, with minor limitations.\n",
        "- 5: Strongly Agree - Excellent across this dimension with negligible gaps.\n",
        "\n",
        "\n",
        "Important:\n",
        "• Use these exact values.\n",
        "• Most good—but not exceptional—answers will earn 3 (“Neither Agree nor Disagree”).\n",
        "• Reserve 5 for *clear* excellence with no substantial weaknesses.\n",
        "• Use half-steps sparingly (e.g., 4 = “Somewhat Agree”) when the solution\n",
        "  almost—but not fully—reaches the next level.\n",
        "• Give low scores to solutions that could be significantly improved\n",
        "• If information needed to assess a point is absent, assume it is weak and\n",
        "  score downward.\n",
        "• A solution that fails to provide detailed and comprehensive answers to all required elements and demonstrates aspirational rather than evidence-based claims should receive low scores.\n",
        "• When unsure, err on the side of the lower score.\n",
        "• If any substantial part of the application is not in English, it should be immediately given low scores.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kgDKds6quGuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**JSON Output Format simultaneous or sequential**"
      ],
      "metadata": {
        "id": "5_UBlENtv14P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Universal JSON Output Format Snippet - Used for simultaneous evaluation (all criteria at once)\n",
        "JSON_OUTPUT_FORMAT_PROMPT_SNIPPET = \"\"\"\n",
        "Your response MUST be in a single JSON object. Do not include any text, explanation, or markdown before or after the JSON. The JSON should have the following top-level keys: \"potential_of_impact\", \"feasibility\", and \"innovative_approach\".\n",
        "\n",
        "Here is the expected JSON structure with definitions for each criterion:\n",
        "{{\n",
        "  \"potential_of_impact\": {{\n",
        "    \"detailed_reasoning\": \"Your highly detailed step-by-step reasoning and assessment for Potential for Impact. Evaluate thoroughly.\",\n",
        "    \"summary_rationale\": \"Summary reasoning in less than 200 words. Be helpful, clear, and highly informative. Provide specific supported evidence from the solution that an external reviewer could check to verify your assessment. Describe both sides if an equal case can be made for both pass or fail.\",\n",
        "    \"passing_probability\": \"Integer between 1-5 using the passing probability scale\"\n",
        "  }},\n",
        "  \"feasibility\": {{\n",
        "    \"detailed_reasoning\": \"Your highly detailed step-by-step reasoning and assessment for Feasibility. Evaluate thoroughly.\",\n",
        "    \"summary_rationale\": \"Summary reasoning in less than 200 words. Be helpful, clear, and extremely informative. Provide specific supported evidence from the solution that an external reviewer could check to verify your assessment. Describe both sides if an equal case can be made for both pass or fail.\",\n",
        "    \"passing_probability\": \"Integer between 1-5 using the passing probability scale\"\n",
        "  }},\n",
        "  \"innovative_approach\": {{\n",
        "    \"detailed_reasoning\": \"Your highly detailed step-by-step reasoning and assessment for Innovative Approach. Evaluate thoroughly.\",\n",
        "    \"summary_rationale\": \"Summary reasoning in less than 200 words. Be helpful, clear, and highly informative. Provide specific supported evidence from the solution that an external reviewer could check to verify your assessment. Describe both sides if an equal case can be made for both pass or fail.\",\n",
        "    \"passing_probability\": \"Integer between 1-5 representing the overall score for Innovative Approach, averaging any relevant sub-scores if applicable.\"\n",
        "  }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# JSON Output Format for Sequential Evaluation (Single Criterion at a time)\n",
        "JSON_OUTPUT_FORMAT_SEQUENTIAL_SINGLE = \"\"\"\n",
        "Your response MUST be in a single JSON object for the ONE criterion being evaluated.\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "- You have been shown all three criteria for context\n",
        "- You must evaluate and score ONLY the ONE criterion specified above\n",
        "- Do NOT provide scores, reasoning, or rationale for the other two criteria\n",
        "- Do NOT include the other criteria names in your JSON response\n",
        "- Your JSON should contain ONLY the evaluation for the requested criterion\n",
        "\n",
        "Do not include any text, explanation, or markdown before or after the JSON.\n",
        "\n",
        "Here is the expected JSON structure:\n",
        "{{\n",
        "  \"detailed_reasoning\": \"Your highly detailed step-by-step reasoning and assessment for THIS SPECIFIC criterion only. Evaluate thoroughly but focus only on the requested criterion.\",\n",
        "  \"summary_rationale\": \"Summary reasoning in less than 200 words for this criterion only. Be helpful, clear, and highly informative. Provide specific supported evidence from the solution.\",\n",
        "  \"passing_probability\": \"Integer between 1-5 using the passing probability scale for this criterion only\"\n",
        "}}\n",
        "\n",
        "Remember: Evaluate ONLY the one criterion specified. Ignore the others.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xEEjaaAlu-zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Common Prompt Components into Independent Variables**"
      ],
      "metadata": {
        "id": "oNZaVlArwOj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# These are text blocks that are shared or have only minor differences between different prompt forms.\n",
        "\n",
        "# Core part of the base system message\n",
        "BASE_SYSTEM_MESSAGE_CORE = \"\"\"\n",
        "You are a finalist selection evaluator for MIT Solve 2025 Global Health Challenge.\n",
        "Your role is to assess each solution against the three official judging criteria described below.\n",
        "Assess each aspect according to the criterion specified.\n",
        "\n",
        "You will receive:\n",
        "- the instructions,\n",
        "- the challenge description,\n",
        "- the criterion to evaluate,\n",
        "- one solution example that gets high scores,\n",
        "- one solution example that gets low scores,\n",
        "- the solution that you are going to evaluate,\n",
        "- the required format of your answer output.\n",
        "\n",
        "Your primary responsibility is to identify the best solutions for the finalists.\n",
        "\"\"\"\n",
        "\n",
        "# Harsh part of the system message\n",
        "HARSH_SYSTEM_MESSAGE_PART = \"\"\"\n",
        "Many solutions (>50%) should get low scores. Your default assumption should be that a solution will not get high scores unless it clearly demonstrates excellence.\n",
        "\n",
        "CRITICAL INSTRUCTION: You must evaluate each criterion INDEPENDENTLY, STRICTLY, CRITICALLY,FAIRLY, and be DEMANDING in your assessments.\n",
        "\"\"\"\n",
        "\n",
        "# Combined Instruction and Important Evaluation Instructions\n",
        "BASE_INSTRUCTION_CORE = \"\"\"\n",
        "Your task:\n",
        "Overall, you are evaluating solutions to a highly competitive global challenge. Most solutions will have significant weaknesses. Only truly exceptional solutions that thoroughly meet all criteria should be selected. When unsure, err on the side of the lower score.\n",
        "\n",
        "1. Carefully read the criterion and understand what it's asking for.\n",
        "2. Analyze the proposed solution in light of this criterion.\n",
        "3. Determine whether the proposed solution meets the criterion based on its own merits.\n",
        "4. Provide your evaluation, explaining your reasoning step-by-step clearly. You will identify strengths and critically list all the ways in which the solution can be improved with regards to the criterion, including weaknesses, limitations, gaps, or risks. Avoid generic praise.\n",
        "Each application must meet all the elements of the established criterion. A solution needs to provide detailed and comprehensive answers to all required elements and demonstrates evidence-based claims.\n",
        "The application must be entirely in English — this is a strict requirement. The only exception is for cities and locations or potentially relevant information that is not in English. Use your best judgment.\n",
        "5. Provide a grade between 1-5 using the grading scale that will be provided.\n",
        "\n",
        "IMPORTANT EVALUATION INSTRUCTIONS:\n",
        "1. Assess each criterion completely separately from the others\n",
        "2. Do not let strengths in one criterion influence your evaluation of other criteria\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Challenge description (same as BASE_CHALLENGE_DESCRIPTION)\n",
        "BASE_CHALLENGE_DESCRIPTION = \"\"\"\n",
        "Challenge Description: Every person deserves to experience good health and well-being. While there has been some progress towards these goals over the last two decades, much of that progress has now slowed or reversed. Currently, half the world lacks access to comprehensive health services. Two billion people face financial hardship due to out-of-pocket healthcare costs. Inequalities are increasing worldwide, particularly in low- and middle-income countries.\n",
        "\n",
        "Technology and innovation have an important role to play in improving health and well-being for all. New technologies can improve health outcomes and access when deployed effectively. Innovation provides the business models, decreased costs, and community-focused design necessary for lasting change and efficient scale. These changes can appear across healthcare systems and affect many areas of care including primary care, mental health, or infectious diseases.\n",
        "\n",
        "MIT Solve seeks exceptional solutions that leverage technology to increase access to good health and healthcare with a particular interest for 2025 in solutions that address one or more of the following or more generally relate to health equity:\n",
        "- Ensure health-related data is collected ethically and effectively and that AI and other insights are accurate, targeted, and actionable in the real world.\n",
        "- Increase capacity and resilience of health systems, including workforce, supply chains, and other infrastructure.\n",
        "- Increase access to and quality of health services for all communities.\n",
        "\"\"\"\n",
        "\n",
        "# Core part of the base criteria definitions (shared by Impact, Feasibility, Innovative Approach)\n",
        "BASE_CRITERIA_IMPACT_DEFINITION_CORE = \"\"\"\n",
        "Criterion to assess: Potential for Impact\n",
        "\n",
        "The planned solution implementation has the potential to impact the intended population.\n",
        "\n",
        "Consider whether the solution has the potential to impact the intended population. Is the description of how the team expects the solution to impact the problem (the theory of change) logical? Has the team tested the assumptions underlying it? Is there evidence from other contexts that strongly suggests that the solution, in its proposed form, can have the intended impact?\n",
        "\n",
        "Some solution teams that have fully launched their solution may provide evidence from an evaluation of their solution. In these cases, the potential for impact is likely to be high, but note that solutions that haven’t yet been launched or evaluated may also score highly on this criterion.\n",
        "\"\"\"\n",
        "\n",
        "BASE_CRITERIA_FEASIBILITY_DEFINITION_CORE = \"\"\"\n",
        "Criterion to assess: Feasibility\n",
        "\n",
        "The team has a realistic, practical plan for implementing the solution, and it is feasible in the given context.\n",
        "\n",
        "Consider whether it is feasible to implement the solution in the given context. Does the team have a realistic, practical plan for implementation that takes into account the political, economic, geographic, and cultural realities in the context? Assuming the necessary funding is acquired, do the necessary conditions exist for the team to carry out their plan? Since many solutions are early-stage, we know that plans will change, so consider whether the team has what it takes - in terms of grit, determination, and expertise - to succeed. This criterion is not about whether the team can attract the funding they need to scale, but rather about the practicalities of implementing the solution.\n",
        "\"\"\"\n",
        "\n",
        "BASE_CRITERIA_INNOVATIVE_APPROACH_DEFINITION_CORE = \"\"\"\n",
        "Criterion to assess: Innovative Approach\n",
        "\n",
        "The solution includes a new technology, a new application of technology, a new business model, or a new process for solving the Challenge.\n",
        "\n",
        "Consider whether the team is proposing a new or significantly improved approach to address the Challenge. Is the solution leveraging a new technology, a new application of an existing technology, or a new business model or process? If it leverages an existing technology or business model, does it involve novel context-specific modifications or methods? Is it sufficiently different from what competitors in the market are offering? Does the team convincingly explain how the solution could change the market or enable broader positive impacts from others in this space?\n",
        "\n",
        "Even if a technology already exists in one environment (e.g. urban areas), we consider the expansion and appropriate adaptation of an existing technology in a new environment (e.g. rural areas) to be innovative.\n",
        "\"\"\"\n",
        "\n",
        "# Rubric part within criteria definitions (shared by Impact, Feasibility, Innovative Approach)\n",
        "RUBRIC_CRITERIA_PART_IMPACT = \"\"\"\n",
        "A solution should score lower on Potential for Impact if the theory of how it could change lives does not make logical sense, or if there is existing evidence that it will not work.\n",
        "\n",
        "A solution should score higher on Potential for Impact if the theory of how it could change the lives of the intended population makes sense and the applicant provides evidence that it is likely to have the intended impact (either from evaluations of the solution itself or from an existing body of evidence about similar interventions).\n",
        "\"\"\"\n",
        "\n",
        "RUBRIC_CRITERIA_PART_FEASIBILITY = \"\"\"\n",
        "A solution should score lower on Feasibility if the team does not have a realistic plan for implementation, or if the plan is unlikely to succeed (even if funding is acquired).\n",
        "\n",
        "A solution should score higher on Feasibility if the team has a realistic plan for implementation that accounts for the political, economic, geographic, and cultural context, and the team has the necessary skills to implement that plan.\n",
        "\"\"\"\n",
        "\n",
        "RUBRIC_CRITERIA_PART_INNOVATIVE_APPROACH = \"\"\"\n",
        "A solution should score lower on Innovative Approach if it is an implementation of an existing approach without any context-specific modifications.\n",
        "\n",
        "A solution should score higher on Innovative Approach if it is a truly novel approach, or a novel, context-appropriate application of an existing approach\n",
        "\"\"\"\n",
        "\n",
        "# Focus part within criteria definitions (shared by Impact, Feasibility, Innovative Approach)\n",
        "BASE_CRITERIA_FOCUS_SUFFIX_IMPACT = \"\"\"\n",
        "Focus on these aspects of the solution in your evaluation:\n",
        "- What specific problem are you solving?\n",
        "- What is your solution?\n",
        "- Who does your solution serve, and in what ways will the solution impact their lives? OR Which Indigenous community(s) does your solution benefit? In what ways will your solution benefit this community?\n",
        "- Describe in simple terms how and why you expect your solution to have an impact on the problem.\n",
        "\"\"\"\n",
        "\n",
        "BASE_CRITERIA_FOCUS_SUFFIX_FEASIBILITY = \"\"\"\n",
        "Focus on these aspects of the solution in your evaluation:\n",
        "- What type of organization is your solution team?\n",
        "- How are you and your team well-positioned to deliver this solution?\n",
        "- What are your impact goals for your solution and how are you measuring your progress towards them?\n",
        "- In which countries do you currently operate? OR In which parts of the US and/or Canada do you currently operate?\n",
        "- Which, if any, additional countries will you be operating in within the next year? OR Which, if any, additional parts of the US or Canada will you be operating in within the next year?\n",
        "- What is your business model?\n",
        "\"\"\"\n",
        "\n",
        "BASE_CRITERIA_FOCUS_SUFFIX_INNOVATIVE_APPROACH = \"\"\"\n",
        "Focus on these aspects of the solution in your evaluation:\n",
        "- Which of the following categories best describes your solution?\n",
        "- What makes your solution innovative?\n",
        "- Describe the core technology that powers your solution.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "wUUlydGLg6Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt Content Configuration"
      ],
      "metadata": {
        "id": "QJclnUzxg6xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This dictionary stores all the unique prompt components for each defined prompt form.\n",
        "# Each form has its own system_message, instruction, challenge_description, criteria definitions,\n",
        "# and flags to control the inclusion of dynamic examples.\n",
        "\n",
        "PROMPT_CONTENT_CONFIG = {\n",
        "    \"default\": {\n",
        "        \"system_message\": BASE_SYSTEM_MESSAGE_CORE + HARSH_SYSTEM_MESSAGE_PART, # Core + Harsh part\n",
        "        \"instruction\": BASE_INSTRUCTION_CORE,\n",
        "        \"challenge_description\": BASE_CHALLENGE_DESCRIPTION, # Directly reference common challenge description\n",
        "        \"criteria_impact_definition\": BASE_CRITERIA_IMPACT_DEFINITION_CORE + RUBRIC_CRITERIA_PART_IMPACT + BASE_CRITERIA_FOCUS_SUFFIX_IMPACT,\n",
        "        \"criteria_feasibility_definition\": BASE_CRITERIA_FEASIBILITY_DEFINITION_CORE + RUBRIC_CRITERIA_PART_FEASIBILITY + BASE_CRITERIA_FOCUS_SUFFIX_FEASIBILITY,\n",
        "        \"criteria_innovative_approach_definition\": BASE_CRITERIA_INNOVATIVE_APPROACH_DEFINITION_CORE + RUBRIC_CRITERIA_PART_INNOVATIVE_APPROACH + BASE_CRITERIA_FOCUS_SUFFIX_INNOVATIVE_APPROACH,\n",
        "        \"use_dynamic_few_shot_examples\": True # Include dynamic examples based on example_dict\n",
        "    },\n",
        "    \"no_harsh\": {\n",
        "        \"system_message\": BASE_SYSTEM_MESSAGE_CORE, # Remove harsh part, keep core\n",
        "        \"instruction\": BASE_INSTRUCTION_CORE,\n",
        "        \"challenge_description\": BASE_CHALLENGE_DESCRIPTION,\n",
        "        \"criteria_impact_definition\": BASE_CRITERIA_IMPACT_DEFINITION_CORE + RUBRIC_CRITERIA_PART_IMPACT + BASE_CRITERIA_FOCUS_SUFFIX_IMPACT,\n",
        "        \"criteria_feasibility_definition\": BASE_CRITERIA_FEASIBILITY_DEFINITION_CORE + RUBRIC_CRITERIA_PART_FEASIBILITY + BASE_CRITERIA_FOCUS_SUFFIX_FEASIBILITY,\n",
        "        \"criteria_innovative_approach_definition\": BASE_CRITERIA_INNOVATIVE_APPROACH_DEFINITION_CORE + RUBRIC_CRITERIA_PART_INNOVATIVE_APPROACH + BASE_CRITERIA_FOCUS_SUFFIX_INNOVATIVE_APPROACH,\n",
        "        \"use_dynamic_few_shot_examples\": True\n",
        "    },\n",
        "    \"no_rubric\": {\n",
        "        \"system_message\": BASE_SYSTEM_MESSAGE_CORE + HARSH_SYSTEM_MESSAGE_PART,\n",
        "        \"instruction\": BASE_INSTRUCTION_CORE,\n",
        "        \"challenge_description\": BASE_CHALLENGE_DESCRIPTION,\n",
        "        \"criteria_impact_definition\": BASE_CRITERIA_IMPACT_DEFINITION_CORE + BASE_CRITERIA_FOCUS_SUFFIX_IMPACT, # Remove Rubric part\n",
        "        \"criteria_feasibility_definition\": BASE_CRITERIA_FEASIBILITY_DEFINITION_CORE + BASE_CRITERIA_FOCUS_SUFFIX_FEASIBILITY, # Remove Rubric part\n",
        "        \"criteria_innovative_approach_definition\": BASE_CRITERIA_INNOVATIVE_APPROACH_DEFINITION_CORE + BASE_CRITERIA_FOCUS_SUFFIX_INNOVATIVE_APPROACH, # Remove Rubric part\n",
        "        \"use_dynamic_few_shot_examples\": True\n",
        "    },\n",
        "    \"no_examples\": { # New form: no dynamic examples, but had static EXAMPLES_RATIONALES (now removed)\n",
        "\n",
        "        \"system_message\": BASE_SYSTEM_MESSAGE_CORE + HARSH_SYSTEM_MESSAGE_PART,\n",
        "        \"instruction\": BASE_INSTRUCTION_CORE,\n",
        "        \"challenge_description\": BASE_CHALLENGE_DESCRIPTION,\n",
        "        \"criteria_impact_definition\": BASE_CRITERIA_IMPACT_DEFINITION_CORE + RUBRIC_CRITERIA_PART_IMPACT + BASE_CRITERIA_FOCUS_SUFFIX_IMPACT,\n",
        "        \"criteria_feasibility_definition\": BASE_CRITERIA_FEASIBILITY_DEFINITION_CORE + RUBRIC_CRITERIA_PART_FEASIBILITY + BASE_CRITERIA_FOCUS_SUFFIX_FEASIBILITY,\n",
        "        \"criteria_innovative_approach_definition\": BASE_CRITERIA_INNOVATIVE_APPROACH_DEFINITION_CORE + RUBRIC_CRITERIA_PART_INNOVATIVE_APPROACH + BASE_CRITERIA_FOCUS_SUFFIX_INNOVATIVE_APPROACH,\n",
        "        \"use_dynamic_few_shot_examples\": False # Explicitly exclude dynamic examples\n",
        "    },\n",
        "    \"without_everything\": {\n",
        "\n",
        "        \"system_message\": BASE_SYSTEM_MESSAGE_CORE,\n",
        "        \"instruction\": BASE_INSTRUCTION_CORE,\n",
        "        \"challenge_description\": BASE_CHALLENGE_DESCRIPTION,\n",
        "        \"criteria_impact_definition\": BASE_CRITERIA_IMPACT_DEFINITION_CORE + BASE_CRITERIA_FOCUS_SUFFIX_IMPACT,\n",
        "        \"criteria_feasibility_definition\": BASE_CRITERIA_FEASIBILITY_DEFINITION_CORE + BASE_CRITERIA_FOCUS_SUFFIX_FEASIBILITY,\n",
        "        \"criteria_innovative_approach_definition\": BASE_CRITERIA_INNOVATIVE_APPROACH_DEFINITION_CORE + BASE_CRITERIA_FOCUS_SUFFIX_INNOVATIVE_APPROACH,\n",
        "        \"use_dynamic_few_shot_examples\": False # Explicitly exclude dynamic examples\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Ju0uhEpuguxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OpenAI API Configuration"
      ],
      "metadata": {
        "id": "abrrMWZXhFfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. OpenAI API Configuration ---\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "global openai_client\n",
        "openai_client = None\n",
        "selected_openai_model_name = \"gpt-5-nano\"\n",
        "\n",
        "def configure_openai_client():\n",
        "    \"\"\"Initializes the OpenAI client using an environment variable.\"\"\"\n",
        "    global openai_client\n",
        "    try:\n",
        "        openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not openai_api_key:\n",
        "            print(\"OPENAI_API_KEY not found in environment variables. Please set it.\")\n",
        "            if 'google.colab' in str(get_ipython()):\n",
        "                print(\"In Google Colab, consider setting it as a secret: from google.colab import userdata; os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\")\n",
        "\n",
        "        if openai_api_key:\n",
        "            openai_client = OpenAI(api_key=openai_api_key)\n",
        "            print(f\"OpenAI API client initialized successfully using model: {selected_openai_model_name}.\")\n",
        "        else:\n",
        "            print(\"OpenAI API calls will be skipped due to missing API key.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during OpenAI API configuration: {e}\")\n",
        "        print(\"Please ensure the 'openai' library is installed and your API key is correctly configured.\")\n",
        "\n",
        "# Call configuration at script start\n",
        "configure_openai_client()"
      ],
      "metadata": {
        "id": "-JrTt_gqhFxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "JrcMF2JVhLo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data():\n",
        "    \"\"\"Loads data from health.xlsx or uses sample data if not found.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(\"health.xlsx\")\n",
        "        tqdm.write(\"Successfully loaded health.xlsx\")\n",
        "    except FileNotFoundError:\n",
        "        tqdm.write(\"health.xlsx not found. Using dummy sample data for demonstration.\")\n",
        "        sample_data = {\n",
        "            'Solution ID': [1, 2, 3, 4, 5],\n",
        "            'Problem': ['Lack of clean water', 'Poor education access', 'Food waste', 'Mental health stigma', 'Lack of digital literacy'],\n",
        "            'Solution': ['Solar water purifier for rural areas', 'Interactive online learning platform', 'AI-powered food distribution system', 'Community-based mental health support', 'Free coding bootcamps for youth'],\n",
        "            'Description': [\n",
        "                'Our solar water purifier uses advanced filtration to provide clean drinking water to remote villages. It is low-cost, easy to maintain, but requires initial funding for widespread deployment.',\n",
        "                'An adaptive online platform that uses gamification and personalized learning paths to improve educational outcomes for underserved communities. Needs strong internet infrastructure.',\n",
        "                'This system uses machine learning to predict food surplus and efficiently redistribute it to food banks, reducing waste and feeding the needy. Requires partnerships with food producers and retailers.',\n",
        "                'A peer-support network combined with accessible therapy sessions to combat mental health stigma and provide immediate support. Relies heavily on volunteer commitment and community trust.',\n",
        "                'Intensive bootcamps teaching essential coding skills to unemployed youth, preparing them for tech jobs in a rapidly evolving economy.'\n",
        "            ],\n",
        "            'Team': ['Aqua Innovators', 'EdTech Pioneers', 'ZeroWaste Solutions', 'Mindful Minds', 'CodeUp'],\n",
        "            'Created': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05'],\n",
        "            'Updated': ['2024-06-01', '2024-06-01', '2024-06-01', '2024-06-01', '2024-06-01'],\n",
        "            'User / Team': ['UserA', 'UserB', 'UserC', 'UserD', 'UserE'],\n",
        "            'Name': ['Project Alpha', 'Project Beta', 'Project Gamma', 'Project Delta', 'Project Epsilon'],\n",
        "            'Team Leader': ['LeaderA', 'LeaderB', 'LeaderC', 'LeaderD', 'LeaderE'],\n",
        "            'Team Leader Email': ['a@example.com', 'b@example.com', 'c@example.com', 'd@example.com', 'e@example.com'],\n",
        "            'Status': ['Submitted', 'Submitted', 'Submitted', 'Submitted', 'Submitted'],\n",
        "            'Submitted At': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05'],\n",
        "            'Title': ['Clean Water for All', 'Accessible Education', 'Combatting Food Waste', 'Breaking Mental Health Stigma', 'Digital Skills for Youth'],\n",
        "            'Terms Accepted': ['Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n",
        "        }\n",
        "        df = pd.DataFrame(sample_data)\n",
        "\n",
        "    df_processed = df.copy()\n",
        "    dropped_cols = ['Created', 'Updated', 'User / Team', 'Name',\n",
        "                    'Team Leader', 'Team Leader Email', 'Status', 'Submitted At','Title',\n",
        "                    'Terms Accepted']\n",
        "    existing_dropped_cols = [col for col in dropped_cols if col in df_processed.columns]\n",
        "    df_processed = df_processed.drop(columns=existing_dropped_cols, axis=1)\n",
        "\n",
        "    # Modified combine_columns to include Solution ID in the combined text for logging\n",
        "    def combine_columns(row):\n",
        "        parts = []\n",
        "        if \"Solution ID\" in row.index:\n",
        "            parts.append(f\"Solution ID: {row['Solution ID']}\")\n",
        "\n",
        "        for col in row.index:\n",
        "            if col != \"Solution ID\": # Exclude Solution ID to avoid duplication if already added\n",
        "                parts.append(f\"{col}: {row[col]}\")\n",
        "\n",
        "        return '\\n '.join(parts)\n",
        "\n",
        "    df_processed['Combined'] = df_processed.apply(combine_columns, axis=1)\n",
        "    df_processed[\"Combined\"] = df_processed[\"Combined\"].apply(lambda x: re.sub('<.*?>', '', str(x)))\n",
        "    return df_processed\n"
      ],
      "metadata": {
        "id": "S_2IqWnLhODx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chosen cases"
      ],
      "metadata": {
        "id": "TcyXCrARddf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter the matched case"
      ],
      "metadata": {
        "id": "ZVsGUuk-PtBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file paths\n",
        "file_path_1 = \"health.xlsx\"\n",
        "file_path_2 = \"Health Review Round.xlsx\"\n",
        "\n",
        "try:\n",
        "    # Load the two Excel files into pandas DataFrames\n",
        "    df1 = pd.read_excel(file_path_1)\n",
        "    df2 = pd.read_excel(file_path_2)\n",
        "\n",
        "    # Assuming 'Solution ID' is the column containing the IDs in both files\n",
        "    # Get the Solution IDs from each DataFrame\n",
        "    solution_ids_1 = set(df1['Solution ID'].tolist())\n",
        "    solution_ids_2 = set(df2['Solution ID'].tolist())\n",
        "\n",
        "    # Find the common Solution IDs (intersection of the two sets)\n",
        "    matching_solution_ids = list(solution_ids_1.intersection(solution_ids_2))\n",
        "\n",
        "    # Display the matching Solution IDs\n",
        "    print(\"Matching Solution IDs found in both files:\")\n",
        "    print(matching_solution_ids)\n",
        "\n",
        "    # Filter df2 (Health Review Round.xlsx) based on the matching_solution_ids\n",
        "    filtered_df2 = df2[df2['Solution ID'].isin(matching_solution_ids)].copy()\n",
        "    print(\"\\nFiltered rows from Health Review Round.xlsx based on matching Solution IDs:\")\n",
        "    display(filtered_df2)\n",
        "\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: One of the files not found - {e}\")\n",
        "except KeyError as e:\n",
        "    print(f\"Error: 'Solution ID' column not found in one of the files - {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "_uv6Jj3DPs10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data():\n",
        "    \"\"\"\n",
        "    Loads data from health.xlsx, filters for specific Solution IDs,\n",
        "    and preprocesses the data for LLM input.\n",
        "    \"\"\"\n",
        "    # Define the specific Solution IDs to be included in the evaluation.\n",
        "    # This list ensures that only these nine proposals are processed.\n",
        "    specific_solution_ids = [95105, 93186, 94339, 96129, 97666, 94606, 96399, 95356, 92944, 93842, 95635, 94618, 94620, 93347, 95014, 97703, 93736, 93235, 93367, 93113, 94267, 95296, 97613, 94030, 94674, 96722, 94548, 95958, 97882, 95709, 95838, 93662, 94819, 98021, 96614, 95847, 94181, 95721, 95729, 96631, 97530, 95227, 94972]\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(\"health.xlsx\")\n",
        "        tqdm.write(\"Successfully loaded health.xlsx\")\n",
        "\n",
        "        # Filter the DataFrame to keep only the rows with the specific Solution IDs.\n",
        "        # The .isin() method checks if the value in 'Solution ID' is in the list.\n",
        "        # We use .copy() to prevent a SettingWithCopyWarning in later operations.\n",
        "        df_filtered = df[df['Solution ID'].isin(specific_solution_ids)].copy()\n",
        "\n",
        "        tqdm.write(f\"Filtered to include only the following Solution IDs: {specific_solution_ids}\")\n",
        "        tqdm.write(f\"Number of proposals after filtering: {len(df_filtered)}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        tqdm.write(\"health.xlsx not found. Using dummy sample data for demonstration.\")\n",
        "        # Note: The dummy data below does not contain the specific IDs you requested.\n",
        "        # The code will proceed with this dummy data if the file is not found.\n",
        "        sample_data = {\n",
        "            'Solution ID': [1, 2, 3, 4, 5],\n",
        "            'Problem': ['Lack of clean water', 'Poor education access', 'Food waste', 'Mental health stigma', 'Lack of digital literacy'],\n",
        "            'Solution': ['Solar water purifier for rural areas', 'Interactive online learning platform', 'AI-powered food distribution system', 'Community-based mental health support', 'Free coding bootcamps for youth'],\n",
        "            'Description': [\n",
        "                'Our solar water purifier uses advanced filtration to provide clean drinking water to remote villages. It is low-cost, easy to maintain, but requires initial funding for widespread deployment.',\n",
        "                'An adaptive online platform that uses gamification and personalized learning paths to improve educational outcomes for underserved communities. Needs strong internet infrastructure.',\n",
        "                'This system uses machine learning to predict food surplus and efficiently redistribute it to food banks, reducing waste and feeding the needy. Requires partnerships with food producers and retailers.',\n",
        "                'A peer-support network combined with accessible therapy sessions to combat mental health stigma and provide immediate support. Relies heavily on volunteer commitment and community trust.',\n",
        "                'Intensive bootcamps teaching essential coding skills to unemployed youth, preparing them for tech jobs in a rapidly evolving economy.'\n",
        "            ],\n",
        "            'Team': ['Aqua Innovators', 'EdTech Pioneers', 'ZeroWaste Solutions', 'Mindful Minds', 'CodeUp'],\n",
        "            'Created': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05'],\n",
        "            'Updated': ['2024-06-01', '2024-06-01', '2024-06-01', '2024-06-01', '2024-06-01'],\n",
        "            'User / Team': ['UserA', 'UserB', 'UserC', 'UserD', 'UserE'],\n",
        "            'Name': ['Project Alpha', 'Project Beta', 'Project Gamma', 'Project Delta', 'Project Epsilon'],\n",
        "            'Team Leader': ['LeaderA', 'LeaderB', 'LeaderC', 'LeaderD', 'LeaderE'],\n",
        "            'Team Leader Email': ['a@example.com', 'b@example.com', 'c@example.com', 'd@example.com', 'e@example.com'],\n",
        "            'Status': ['Submitted', 'Submitted', 'Submitted', 'Submitted', 'Submitted'],\n",
        "            'Submitted At': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05'],\n",
        "            'Title': ['Clean Water for All', 'Accessible Education', 'Combatting Food Waste', 'Breaking Mental Health Stigma', 'Digital Skills for Youth'],\n",
        "            'Terms Accepted': ['Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n",
        "        }\n",
        "        df_filtered = pd.DataFrame(sample_data)\n",
        "\n",
        "    df_processed = df_filtered.copy()\n",
        "    dropped_cols = ['Created', 'Updated', 'User / Team', 'Name',\n",
        "                    'Team Leader', 'Team Leader Email', 'Status', 'Submitted At','Title',\n",
        "                    'Terms Accepted']\n",
        "    existing_dropped_cols = [col for col in dropped_cols if col in df_processed.columns]\n",
        "    df_processed = df_processed.drop(columns=existing_dropped_cols, axis=1, errors='ignore')\n",
        "\n",
        "    # Modified combine_columns to include Solution ID in the combined text for logging\n",
        "    def combine_columns(row):\n",
        "        parts = []\n",
        "        if \"Solution ID\" in row.index:\n",
        "            parts.append(f\"Solution ID: {row['Solution ID']}\")\n",
        "\n",
        "        for col in row.index:\n",
        "            if col != \"Solution ID\": # Exclude Solution ID to avoid duplication if already added\n",
        "                parts.append(f\"{col}: {row[col]}\")\n",
        "\n",
        "        return '\\n '.join(parts)\n",
        "\n",
        "    df_processed['Combined'] = df_processed.apply(combine_columns, axis=1)\n",
        "    df_processed[\"Combined\"] = df_processed[\"Combined\"].apply(lambda x: re.sub('<.*?>', '', str(x)))\n",
        "    return df_processed\n"
      ],
      "metadata": {
        "id": "bF_uT3bCgyGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt Construction Logic (example for score)"
      ],
      "metadata": {
        "id": "-kTEdPin6w1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function is the core of dynamic prompt generation based on the selected form.\n",
        "\n",
        "def get_prompts_for_form(prompt_form_name, example_dict=None):\n",
        "    \"\"\"\n",
        "    Retrieves and constructs system and user prompts based on the specified prompt form.\n",
        "    It fetches content from the PROMPT_CONTENT_CONFIG and dynamically includes examples if provided.\n",
        "\n",
        "    Args:\n",
        "        prompt_form_name (str): The name of the prompt form.\n",
        "        example_dict (dict, optional): Dictionary containing example solution texts keyed by type\n",
        "                                        (e.g., 'pass_all', 'criteria1_fail').\n",
        "\n",
        "    Returns:\n",
        "        tuple: (system_prompt_content, user_prompt_template_content, response_is_json_expected).\n",
        "    \"\"\"\n",
        "    config = PROMPT_CONTENT_CONFIG.get(prompt_form_name)\n",
        "    if not config:\n",
        "        raise ValueError(f\"Prompt form '{prompt_form_name}' not found in PROMPT_CONTENT_CONFIG.\")\n",
        "\n",
        "    system_msg = config[\"system_message\"]\n",
        "    instruction_txt = config[\"instruction\"]\n",
        "    challenge_desc = config[\"challenge_description\"]\n",
        "    use_dynamic_examples = config[\"use_dynamic_few_shot_examples\"]\n",
        "\n",
        "    system_prompt_content = \"\"\n",
        "    user_prompt_template_content = \"\"\n",
        "    response_is_json_expected = True # All forms now aim for unified JSON output\n",
        "\n",
        "    COMMON_USER_PROMPT_ALL_CRITERIA_TEMPLATE = f\"\"\"\n",
        "Evaluate the following startup proposal against all three criteria (Potential for Impact, Feasibility, and Innovative Approach).\n",
        "Provide your comprehensive evaluation in the specified JSON format.\n",
        "\n",
        "Startup Proposal:\n",
        "{{combined_proposal_text}}\n",
        "\n",
        "---\n",
        "{{criteria_impact_def}}\n",
        "---\n",
        "{{criteria_feasibility_def}}\n",
        "---\n",
        "{{criteria_innovative_approach_def}}\n",
        "\"\"\"\n",
        "\n",
        "    # --- Constructing System Prompt ---\n",
        "    # Start with the base components for the current form\n",
        "    system_prompt_content = f\"\"\"\n",
        "{system_msg}\n",
        "{instruction_txt}\n",
        "{SCORING_SCALE_TEXT}\n",
        "{challenge_desc}\n",
        "\"\"\"\n",
        "\n",
        "    # Dynamically add few-shot examples if required and provided\n",
        "    examples_section_dynamic = \"\"\n",
        "    examples_reminder_dynamic = \"\"\n",
        "    if use_dynamic_examples and example_dict and any(example_dict.values()):\n",
        "        examples_section_dynamic += \"\\n--- Examples for Reference ---\\nIMPORTANT: The following examples are for reference only. Do not directly compare the proposed solution to these examples. Instead, use them to understand the criteria and how they should be applied.\\n\\n\"\n",
        "\n",
        "        # Add high-scoring examples (scores 4-5)\n",
        "        if 'high_scores' in example_dict and example_dict['high_scores']:\n",
        "            for sol_id, sol_text in example_dict['high_scores']:\n",
        "                examples_section_dynamic += f\"Example of high-scoring solution (ID: {sol_id}) - typically scores 5 across criteria: \\n{sol_text}\\n\\n\"\n",
        "\n",
        "        # Add medium-scoring examples (scores 3)\n",
        "        if 'medium_scores' in example_dict and example_dict['medium_scores']:\n",
        "            for sol_id, sol_text in example_dict['medium_scores']:\n",
        "                examples_section_dynamic += f\"Example of medium-scoring solution (ID: {sol_id}) - typically scores 3 across criteria: \\n{sol_text}\\n\\n\"\n",
        "\n",
        "        # Add overall low-scoring examples (scores 1-2 across multiple criteria)\n",
        "        if 'low_scores' in example_dict and example_dict['low_scores']:\n",
        "            for sol_id, sol_text in example_dict['low_scores']:\n",
        "                examples_section_dynamic += f\"Example of low-scoring solution overall (ID: {sol_id}) - typically scores 2 across multiple criteria: \\n{sol_text}\\n\\n\"\n",
        "\n",
        "        # Add low-scoring examples for specific criteria (scores 1-2)\n",
        "        # Map our 3 criteria names to keys used in example_dict\n",
        "        criterion_keys_in_examples = {\n",
        "            \"potential_of_impact\": \"criteria1_low_score\",\n",
        "            \"feasibility\": \"criteria2_low_score\",\n",
        "            \"innovative_approach\": \"criteria3_low_score\"\n",
        "        }\n",
        "        for criterion_name, example_key in criterion_keys_in_examples.items():\n",
        "            if example_key in example_dict and example_dict[example_key]:\n",
        "                for sol_id, sol_text in example_dict[example_key]:\n",
        "                    examples_section_dynamic += f\"Example with low score on {criterion_name} (ID: {sol_id}) - typically scores 1-2 on this criterion: \\n{sol_text}\\n\\n\"\n",
        "\n",
        "        #examples_reminder_dynamic = \"\\nREMINDER: The above examples are just to illustrate the 5-point scoring scale and criteria application. Your evaluation should be based solely on how well the proposed solution meets the given criteria, not on how similar it is to the examples. Use the full 1-5 scoring scale: 1 (Strongly Disagree), 2 (Somewhat Disagree), 3 (Neither Agree nor Disagree), 4 (Somewhat Agree), 5 (Strongly Agree). Evaluate each criterion independently and assign scores based on the solution's actual merits.\\n\"\n",
        "\n",
        "        examples_reminder_dynamic = \"\"\"REMINDER: The above examples are provided to clarify the extreme ends of the 5-point scoring scale and illustrate the type of proposals that receive very high or very low scores. Specifically:\n",
        "- Solutions marked as 'high_scores' are genuinely excellent proposals that meet all criteria exceptionally well. These proposals should serve as the benchmark for a score of 5.\n",
        "- Solutions marked as 'low_scores' or 'criteria_low_score' are genuinely weak proposals with significant, demonstrable flaws on a specific criterion or across the board. These proposals should serve as the benchmark for scores of 1 or 2.\n",
        "Your evaluation must be based solely on the merits of the proposed solution you are currently assessing. Do not assign high scores unless the solution is truly outstanding and clearly meets the high-scoring criteria demonstrated by the examples. Conversely, you MUST be prepared to assign low scores (1s and 2s) if the solution is weak, as illustrated in the examples.\n",
        "You must use the full 1-5 scoring scale: 1 (Strongly Disagree), 2 (Somewhat Disagree), 3 (Neither Agree nor Disagree), 4 (Somewhat Agree), 5 (Strongly Agree). Evaluate each criterion independently and assign scores based on the solution's actual merits. Do not be generous with your scoring.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        system_prompt_content += examples_section_dynamic\n",
        "        system_prompt_content += examples_reminder_dynamic\n",
        "\n",
        "\n",
        "    # Add JSON output snippet (all forms now aim for JSON output)\n",
        "    system_prompt_content += f\"{JSON_OUTPUT_FORMAT_PROMPT_SNIPPET}\"\n",
        "\n",
        "    # Add form-specific final instruction/clarification to system prompt\n",
        "    # These are specific instructions that guide the model's final behavior/tone.\n",
        "    if prompt_form_name == \"default\":\n",
        "        system_prompt_content += \"Remember the IMPORTANT EVALUATION INSTRUCTIONS. Be critical, rigorous, and demanding in your assessments.\\n\"\n",
        "    elif prompt_form_name == \"no_harsh\":\n",
        "        system_prompt_content += \"Remember the IMPORTANT EVALUATION INSTRUCTIONS. (Note: This version is less harsh in its tone compared to 'default' in some aspects, but still rigorous.)\\n\"\n",
        "    elif prompt_form_name == \"no_rubric\":\n",
        "        system_prompt_content += \"You are an evaluator for startup proposals. Provide your assessment for all three criteria in the specified JSON format.\\n\"\n",
        "    elif prompt_form_name == \"no_examples\":\n",
        "        system_prompt_content += \"You are an evaluator for startup proposals. Focus solely on the provided instructions and challenge description. Do not generate examples.\"\n",
        "        system_prompt_content += \"\\nProvide your assessment in the specified JSON format.\\n\"\n",
        "    elif prompt_form_name == \"without_everything\":\n",
        "        system_prompt_content += \"Provide your assessment in the specified JSON format.\"\n",
        "\n",
        "\n",
        "    # --- Constructing User Prompt ---\n",
        "    # Populate the common user prompt template with criterion definitions specific to the current form\n",
        "    user_prompt_template_content = COMMON_USER_PROMPT_ALL_CRITERIA_TEMPLATE.format(\n",
        "        combined_proposal_text=\"{combined_proposal_text}\", # This remains as a placeholder to be formatted later\n",
        "        criteria_impact_def=config[\"criteria_impact_definition\"],\n",
        "        criteria_feasibility_def=config[\"criteria_feasibility_definition\"],\n",
        "        criteria_innovative_approach_def=config[\"criteria_innovative_approach_definition\"]\n",
        "    )\n",
        "\n",
        "    return system_prompt_content, user_prompt_template_content, response_is_json_expected"
      ],
      "metadata": {
        "id": "h7SJGy1V6wup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenAI Evaluation API Call Function for SIMULTANEOUS evaluation**"
      ],
      "metadata": {
        "id": "eJVdE-65hXVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function executes a single API call for one proposal.\n",
        "\n",
        "def perform_openai_api_call(combined_text, prompt_form_name, example_dict=None):\n",
        "    \"\"\"\n",
        "    Executes a single API call to OpenAI GPT-4o-mini for evaluation.\n",
        "    All forms now attempt a single unified API call expecting JSON.\n",
        "    Dynamically includes examples in the prompt if example_dict is provided.\n",
        "    Includes robust retry logic for API errors.\n",
        "    \"\"\"\n",
        "    # Check if OpenAI client is initialized globally\n",
        "    if openai_client is None:\n",
        "        tqdm.write(f\"Skipping OpenAI API call for {prompt_form_name}. API client not initialized.\")\n",
        "        # Return a dummy structure that matches the expected unified JSON output\n",
        "        return {\n",
        "            \"feasibility\": {\"detailed_reasoning\": \"API skipped.\", \"summary_rationale\": \"API skipped.\", \"passing_probability\": None},\n",
        "            \"potential_of_impact\": {\"detailed_reasoning\": \"API skipped.\", \"summary_rationale\": \"API skipped.\", \"passing_probability\": None},\n",
        "            \"innovative_approach\": {\"detailed_reasoning\": \"API skipped.\", \"summary_rationale\": \"API skipped.\", \"passing_probability\": None},\n",
        "            \"raw_response_content\": \"API skipped due to no client.\"\n",
        "        }\n",
        "\n",
        "    # Retrieve prompt components specific to the form and with examples if applicable\n",
        "    system_prompt, user_prompt_template, response_is_json_expected = get_prompts_for_form(prompt_form_name, example_dict=example_dict)\n",
        "    # Format the user prompt with the actual proposal text\n",
        "    # The {combined_proposal_text} in user_prompt_template will now be correctly replaced.\n",
        "    full_user_prompt = user_prompt_template.format(combined_proposal_text=combined_text)\n",
        "\n",
        "    # Assemble the messages for the API call\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": full_user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Log the API call action, using tqdm.write to avoid interfering with progress bar\n",
        "    # Extract Solution ID safely for logging\n",
        "    solution_id_match = re.search(r'Solution ID: (\\d+)', combined_text)\n",
        "    solution_id_for_log = solution_id_match.group(1) if solution_id_match else \"N/A\"\n",
        "    tqdm.write(f\"Making OpenAI API call for proposal ID {solution_id_for_log} (Form: {prompt_form_name})...\")\n",
        "\n",
        "\n",
        "    # --- API Call with Retry Logic ---\n",
        "    max_retries = 5\n",
        "    base_delay = 1 # seconds\n",
        "    import time # Import time for sleep function\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Prepare arguments for the chat completion API call\n",
        "            completion_args = {\n",
        "                \"model\": selected_openai_model_name,\n",
        "                \"messages\": messages,\n",
        "                \"temperature\": EXPERIMENT_TEMPERATURE # A common temperature for balanced creativity and consistency\n",
        "            }\n",
        "            # Explicitly request JSON response format, as all forms are designed to output it now\n",
        "            completion_args['response_format'] = {\"type\": \"json_object\"}\n",
        "\n",
        "            # Execute the API call\n",
        "            response = openai_client.chat.completions.create(**completion_args)\n",
        "            # Extract the content from the API response\n",
        "\n",
        "            total_tokens = response.usage.total_tokens if response.usage else 0\n",
        "            tqdm.write(f\"API call for {solution_id_for_log} used {total_tokens} tokens.\")\n",
        "\n",
        "            response_content = response.choices[0].message.content if response.choices and response.choices[0].message else \"\"\n",
        "\n",
        "            # Attempt to parse the response content as JSON\n",
        "            try:\n",
        "                parsed_response = json.loads(response_content)\n",
        "                # Return the parsed JSON along with the raw content\n",
        "                return {\n",
        "                    \"feasibility\": parsed_response.get('feasibility', {}),\n",
        "                    \"potential_of_impact\": parsed_response.get('potential_of_impact', {}),\n",
        "                    \"innovative_approach\": parsed_response.get('innovative_approach', {}),\n",
        "                    \"raw_response_content\": response_content\n",
        "                }\n",
        "            except json.JSONDecodeError as json_e: # Capture the JSONDecodeError object\n",
        "                # If JSON decoding fails, log the error and provide a fallback structure.\n",
        "                # This might happen if the model fails to adhere to the JSON format, especially with minimal instructions.\n",
        "                tqdm.write(f\"JSONDecodeError for proposal ID {solution_id_for_log} (Form: {prompt_form_name}). Error: {json_e}. Raw response: {response_content[:200]}...\")\n",
        "\n",
        "                # Heuristically attempt to extract scores from raw text if JSON is malformed\n",
        "                # This is a best-effort approach for cases where the model doesn't return proper JSON.\n",
        "                impact_score = None\n",
        "                feasibility_score = None\n",
        "                innovative_approach_score = None\n",
        "\n",
        "                # Use regex to find potential scores in the unstructured text. This is a best-effort approach.\n",
        "                # Regex patterns for score extraction\n",
        "                impact_match = re.search(r'(?:Impact|potential of impact)(?:\\s*(?:score|passing probability))?:\\s*(\\d(?:\\.\\d)?)', response_content, re.IGNORECASE)\n",
        "                feasibility_match = re.search(r'(?:Feasibility|feasibility)(?:\\s*(?:score|passing probability))?:\\s*(\\d(?:\\.\\d)?)', response_content, re.IGNORECASE)\n",
        "                innovative_match = re.search(r'(?:Innovative Approach|innovative approach)(?:\\s*(?:score|passing probability))?:\\s*(\\d(?:\\.\\d)?)', response_content, re.IGNORECASE)\n",
        "\n",
        "                try:\n",
        "                    if impact_match: impact_score = float(impact_match.group(1))\n",
        "                    if feasibility_match: feasibility_score = float(feasibility_match.group(1))\n",
        "                    if innovative_match: innovative_approach_score = float(innovative_match.group(1))\n",
        "                except ValueError:\n",
        "                    pass # If conversion to float fails (e.g., regex finds non-numeric string), score remains None\n",
        "\n",
        "\n",
        "                # Return a structured error response with any heuristically extracted scores\n",
        "                return {\n",
        "                    \"feasibility\": {\"detailed_reasoning\": f\"JSON parse error or unstructured response: {response_content}\", \"summary_rationale\": f\"JSON parse error or unstructured response: {response_content[:200]}...\", \"passing_probability\": feasibility_score},\n",
        "                    \"potential_of_impact\": {\"detailed_reasoning\": f\"JSON parse error or unstructured response: {response_content}\", \"summary_rationale\": f\"JSON parse error or unstructured response. Raw: {response_content[:200]}...\", \"passing_probability\": impact_score},\n",
        "                    \"innovative_approach\": {\"detailed_reasoning\": f\"JSON parse error or unstructured response: {response_content}\", \"summary_rationale\": f\"JSON parse error or unstructured response. Raw: {response_content[:200]}...\", \"passing_probability\": innovative_approach_score},\n",
        "                    \"raw_response_content\": response_content\n",
        "                }\n",
        "\n",
        "        except (RateLimitError, APIConnectionError, APIStatusError) as e:\n",
        "            # Handle API-specific errors, especially RateLimitError (429)\n",
        "            if isinstance(e, RateLimitError):\n",
        "                tqdm.write(f\"RateLimitError for proposal ID {solution_id_for_log} (Form: {prompt_form_name}) on attempt {attempt + 1}/{max_retries}. Retrying in {base_delay * (2**attempt)} seconds...\")\n",
        "            elif isinstance(e, APIConnectionError):\n",
        "                tqdm.write(f\"APIConnectionError for proposal ID {solution_id_for_log} (Form: {prompt_form_name}) on attempt {attempt + 1}/{max_retries}. Retrying in {base_delay * (2**attempt)} seconds...\")\n",
        "            else: # Other APIStatusError (e.g., 500, 502, 503)\n",
        "                 tqdm.write(f\"APIStatusError for proposal ID {solution_id_for_log} (Form: {prompt_form_name}) on attempt {attempt + 1}/{max_retries}. Error: {e.status_code}. Retrying in {base_delay * (2**attempt)} seconds...\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(base_delay * (2**attempt)) # Exponential backoff\n",
        "                continue # Try again\n",
        "            else:\n",
        "                tqdm.write(f\"Max retries reached for proposal ID {solution_id_for_log} (Form: {prompt_form_name}). Failing...\")\n",
        "                # After max retries, propagate the error or return a failure state\n",
        "                # Here, we return a failure state that gets logged in the DataFrame\n",
        "                return {\n",
        "                    \"feasibility\": {\"detailed_reasoning\": f\"API call failed after retries: {e}\", \"summary_rationale\": f\"API error: {e}\", \"passing_probability\": None},\n",
        "                    \"potential_of_impact\": {\"detailed_reasoning\": f\"API error: {e}\", \"summary_rationale\": f\"API error: {e}\", \"passing_probability\": None},\n",
        "                    \"innovative_approach\": {\"detailed_reasoning\": f\"API error: {e}\", \"summary_rationale\": f\"API error: {e}\", \"passing_probability\": None},\n",
        "                    \"raw_response_content\": f\"API call failed after retries: {e}\"\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Catch any other unexpected exceptions during API call or processing (e.g., unexpected code error)\n",
        "            tqdm.write(f\"An unexpected error occurred for proposal ID {solution_id_for_log} (Form: {prompt_form_name}). Error: {e}\")\n",
        "            # Fail immediately for non-retryable errors or unexpected errors\n",
        "            return {\n",
        "                \"feasibility\": {\"detailed_reasoning\": f\"API call error: {e}\", \"summary_rationale\": f\"API call error: {e}\", \"passing_probability\": None},\n",
        "                \"potential_of_impact\": {\"detailed_reasoning\": f\"API call error: {e}\", \"summary_rationale\": f\"API error: {e}\", \"passing_probability\": None},\n",
        "                \"innovative_approach\": {\"detailed_reasoning\": f\"API call error: {e}\", \"summary_rationale\": f\"API error: {e}\", \"passing_probability\": None},\n",
        "                \"raw_response_content\": f\"API call failed: {e}\"\n",
        "            }"
      ],
      "metadata": {
        "id": "jw3vQLsVhcZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SEQUENTIAL Evaluation Functions**"
      ],
      "metadata": {
        "id": "xIbBnON9yy0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_single_criterion_prompt(criterion_name, combined_text, prompt_form_name, all_criteria_definitions, example_dict=None):\n",
        "    \"\"\"\n",
        "    Build a prompt for evaluating a SINGLE criterion independently.\n",
        "\n",
        "    CRITICAL: Uses the EXACT SAME prompt as simultaneous evaluation (with all 3 criteria definitions),\n",
        "    but explicitly instructs the AI to evaluate ONLY ONE specific criterion.\n",
        "    This allows us to test for halo effect by comparing:\n",
        "    - Simultaneous: AI sees all criteria and evaluates all at once (potential halo effect)\n",
        "    - Sequential: AI sees all criteria but evaluates only one at a time (reduced halo effect)\n",
        "\n",
        "    Args:\n",
        "        criterion_name: Which criterion to evaluate ('potential_of_impact', 'feasibility', 'innovative_approach')\n",
        "        combined_text: The solution proposal text to evaluate\n",
        "        prompt_form_name: The prompt form being used ('default', 'no_harsh', etc.)\n",
        "        all_criteria_definitions: Dict with all 3 criteria definitions (same as simultaneous)\n",
        "        example_dict: Optional dict with few-shot examples (same as simultaneous)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (system_message, user_message) for API call\n",
        "    \"\"\"\n",
        "    # Get the config for this form - SAME as simultaneous\n",
        "    config = PROMPT_CONTENT_CONFIG.get(prompt_form_name)\n",
        "    if not config:\n",
        "        raise ValueError(f\"Prompt form '{prompt_form_name}' not found in PROMPT_CONTENT_CONFIG.\")\n",
        "\n",
        "    system_msg = config[\"system_message\"]\n",
        "    instruction_txt = config[\"instruction\"]\n",
        "    challenge_desc = config[\"challenge_description\"]\n",
        "    use_dynamic_examples = config[\"use_dynamic_few_shot_examples\"]\n",
        "\n",
        "    system_message = \"\"\n",
        "    user_message = \"\"\n",
        "\n",
        "    # Template for user message - SAME structure as simultaneous\n",
        "    SINGLE_CRITERION_USER_PROMPT_TEMPLATE = f\"\"\"\n",
        "Evaluate the following startup proposal against all three criteria (Potential for Impact, Feasibility, and Innovative Approach).\n",
        "Provide your comprehensive evaluation in the specified JSON format.\n",
        "\n",
        "Startup Proposal:\n",
        "{{combined_proposal_text}}\n",
        "\n",
        "---\n",
        "{{criteria_impact_def}}\n",
        "---\n",
        "{{criteria_feasibility_def}}\n",
        "---\n",
        "{{criteria_innovative_approach_def}}\n",
        "\"\"\"\n",
        "\n",
        "    # --- Constructing System Message ---\n",
        "    # Start with the base components for the current form - SAME as simultaneous\n",
        "    system_message = f\"\"\"\n",
        "{system_msg}\n",
        "{instruction_txt}\n",
        "{SCORING_SCALE_TEXT}\n",
        "{challenge_desc}\n",
        "\"\"\"\n",
        "\n",
        "    # Dynamically add few-shot examples if required and provided - SAME as simultaneous\n",
        "    examples_section_dynamic = \"\"\n",
        "    examples_reminder_dynamic = \"\"\n",
        "    if use_dynamic_examples and example_dict and any(example_dict.values()):\n",
        "        examples_section_dynamic += \"\\n--- Examples for Reference ---\\nIMPORTANT: The following examples are for reference only. Do not directly compare the proposed solution to these examples. Instead, use them to understand the criteria and how they should be applied.\\n\\n\"\n",
        "\n",
        "        # Add high-scoring examples (scores 4-5)\n",
        "        if 'high_scores' in example_dict and example_dict['high_scores']:\n",
        "            for sol_id, sol_text in example_dict['high_scores']:\n",
        "                examples_section_dynamic += f\"Example of high-scoring solution (ID: {sol_id}) - typically scores 5 across criteria: \\n{sol_text}\\n\\n\"\n",
        "\n",
        "        # Add medium-scoring examples (scores 3)\n",
        "        if 'medium_scores' in example_dict and example_dict['medium_scores']:\n",
        "            for sol_id, sol_text in example_dict['medium_scores']:\n",
        "                examples_section_dynamic += f\"Example of medium-scoring solution (ID: {sol_id}) - typically scores 3 across criteria: \\n{sol_text}\\n\\n\"\n",
        "\n",
        "        # Add overall low-scoring examples (scores 1-2 across multiple criteria)\n",
        "        if 'low_scores' in example_dict and example_dict['low_scores']:\n",
        "            for sol_id, sol_text in example_dict['low_scores']:\n",
        "                examples_section_dynamic += f\"Example of low-scoring solution overall (ID: {sol_id}) - typically scores 2 across multiple criteria: \\n{sol_text}\\n\\n\"\n",
        "\n",
        "        # Add low-scoring examples for specific criteria (scores 1-2)\n",
        "        # Map our 3 criteria names to keys used in example_dict\n",
        "        criterion_keys_in_examples = {\n",
        "            \"potential_of_impact\": \"criteria1_low_score\",\n",
        "            \"feasibility\": \"criteria2_low_score\",\n",
        "            \"innovative_approach\": \"criteria3_low_score\"\n",
        "        }\n",
        "        for crit_name, example_key in criterion_keys_in_examples.items():\n",
        "            if example_key in example_dict and example_dict[example_key]:\n",
        "                for sol_id, sol_text in example_dict[example_key]:\n",
        "                    examples_section_dynamic += f\"Example with low score on {crit_name} (ID: {sol_id}) - typically scores 1-2 on this criterion: \\n{sol_text}\\n\\n\"\n",
        "\n",
        "        examples_reminder_dynamic = \"\"\"REMINDER: The above examples are provided to clarify the extreme ends of the 5-point scoring scale and illustrate the type of proposals that receive very high or very low scores. Specifically:\n",
        "- Solutions marked as 'high_scores' are genuinely excellent proposals that meet all criteria exceptionally well. These proposals should serve as the benchmark for a score of 5.\n",
        "- Solutions marked as 'low_scores' or 'criteria_low_score' are genuinely weak proposals with significant, demonstrable flaws on a specific criterion or across the board. These proposals should serve as the benchmark for scores of 1 or 2.\n",
        "Your evaluation must be based solely on the merits of the proposed solution you are currently assessing. Do not assign high scores unless the solution is truly outstanding and clearly meets the high-scoring criteria demonstrated by the examples. Conversely, you MUST be prepared to assign low scores (1s and 2s) if the solution is weak, as illustrated in the examples.\n",
        "You must use the full 1-5 scoring scale: 1 (Strongly Disagree), 2 (Somewhat Disagree), 3 (Neither Agree nor Disagree), 4 (Somewhat Agree), 5 (Strongly Agree). Evaluate each criterion independently and assign scores based on the solution's actual merits. Do not be generous with your scoring.\n",
        "\"\"\"\n",
        "\n",
        "        system_message += examples_section_dynamic\n",
        "        system_message += examples_reminder_dynamic\n",
        "\n",
        "    # Add the CRITICAL INSTRUCTION for sequential evaluation\n",
        "    system_message += f\"\\n\\n### CRITICAL INSTRUCTION FOR THIS EVALUATION ###\"\n",
        "    system_message += f\"\\nYou will see ALL THREE criteria below, but you must evaluate ONLY: {criterion_name.upper().replace('_', ' ')}\"\n",
        "    system_message += f\"\\nIgnore the other two criteria completely. Do not score them. Do not mention them.\"\n",
        "    system_message += f\"\\nFocus exclusively on {criterion_name.upper().replace('_', ' ')}, but be aware of the full context.\\n\"\n",
        "\n",
        "    # Add JSON output snippet for single criterion\n",
        "    system_message += f\"{JSON_OUTPUT_FORMAT_SEQUENTIAL_SINGLE}\"\n",
        "\n",
        "    # Add form-specific final instruction/clarification to system message - SAME as simultaneous\n",
        "    # These are specific instructions that guide the model's final behavior/tone.\n",
        "    if prompt_form_name == \"default\":\n",
        "        system_message += \"Remember the IMPORTANT EVALUATION INSTRUCTIONS. Be critical, rigorous, and demanding in your assessments.\\n\"\n",
        "    elif prompt_form_name == \"no_harsh\":\n",
        "        system_message += \"Remember the IMPORTANT EVALUATION INSTRUCTIONS. (Note: This version is less harsh in its tone compared to 'default' in some aspects, but still rigorous.)\\n\"\n",
        "    elif prompt_form_name == \"no_rubric\":\n",
        "        system_message += \"You are an evaluator for startup proposals. Provide your assessment in the specified JSON format.\\n\"\n",
        "    elif prompt_form_name == \"no_examples\":\n",
        "        system_message += \"You are an evaluator for startup proposals. Focus solely on the provided instructions and challenge description. Do not generate examples.\"\n",
        "        system_message += \"\\nProvide your assessment in the specified JSON format.\\n\"\n",
        "    elif prompt_form_name == \"without_everything\":\n",
        "        system_message += \"Provide your assessment in the specified JSON format.\"\n",
        "\n",
        "    # --- Constructing User Message ---\n",
        "    # Populate the user prompt template with criterion definitions - SAME as simultaneous\n",
        "    user_message = SINGLE_CRITERION_USER_PROMPT_TEMPLATE.format(\n",
        "        combined_proposal_text=combined_text,\n",
        "        criteria_impact_def=all_criteria_definitions['potential_of_impact'],\n",
        "        criteria_feasibility_def=all_criteria_definitions['feasibility'],\n",
        "        criteria_innovative_approach_def=all_criteria_definitions['innovative_approach']\n",
        "    )\n",
        "\n",
        "    # Add emphasis on which ONE to evaluate\n",
        "    user_message += f\"\\n### YOUR TASK: EVALUATE ONLY {criterion_name.upper().replace('_', ' ')} ###\\n\\n\"\n",
        "    user_message += f\"While you have seen all three criteria above for context, you must ONLY evaluate and provide a score for:\\n\"\n",
        "    user_message += f\"**{criterion_name.upper().replace('_', ' ')}**\\n\\n\"\n",
        "    user_message += \"Do NOT provide scores or detailed reasoning for the other two criteria.\\n\\n\"\n",
        "\n",
        "    return system_message, user_message\n",
        "\n",
        "def perform_sequential_api_call_single_criterion(combined_text, criterion_name, prompt_form_name, all_criteria_definitions, example_dict=None):\n",
        "    \"\"\"\n",
        "    Make an API call to evaluate a SINGLE criterion.\n",
        "    Part of sequential evaluation method.\n",
        "\n",
        "    IMPORTANT: Uses the EXACT SAME prompt content as simultaneous evaluation,\n",
        "    including few-shot examples if provided, but only asks for evaluation of ONE criterion at a time.\n",
        "\n",
        "    Args:\n",
        "        combined_text: Solution proposal text\n",
        "        criterion_name: Which criterion to evaluate ('potential_of_impact', 'feasibility', 'innovative_approach')\n",
        "        prompt_form_name: Prompt form to use\n",
        "        all_criteria_definitions: Dict with all 3 criteria definitions (ensures same prompt as simultaneous)\n",
        "        example_dict: Optional dict with few-shot examples (same as simultaneous)\n",
        "\n",
        "    Returns:\n",
        "        dict: Result for this single criterion with structure:\n",
        "              {\"detailed_reasoning\": ..., \"summary_rationale\": ..., \"passing_probability\": ...}\n",
        "    \"\"\"\n",
        "    if openai_client is None:\n",
        "        tqdm.write(f\"Skipping API call for {criterion_name}. API client not initialized.\")\n",
        "        return {\"detailed_reasoning\": \"API skipped.\", \"summary_rationale\": \"API skipped.\", \"passing_probability\": None}\n",
        "\n",
        "    # Build the prompt - uses SAME content as simultaneous but asks for only one criterion\n",
        "    system_message, user_message = build_single_criterion_prompt(\n",
        "        criterion_name, combined_text, prompt_form_name, all_criteria_definitions, example_dict\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "\n",
        "    # Extract Solution ID for logging\n",
        "    solution_id_match = re.search(r'Solution ID: (\\d+)', combined_text)\n",
        "    solution_id_for_log = solution_id_match.group(1) if solution_id_match else \"N/A\"\n",
        "\n",
        "    # Log with info about examples\n",
        "    examples_info = \" (with examples)\" if example_dict and any(example_dict.values()) else \" (no examples)\"\n",
        "    tqdm.write(f\"  └─ Evaluating {criterion_name} for proposal ID {solution_id_for_log}{examples_info} (using full prompt, requesting only 1 criterion)...\")\n",
        "\n",
        "    # API Call with retry logic\n",
        "    max_retries = 5\n",
        "    base_delay = 1\n",
        "    import time\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            completion_args = {\n",
        "                \"model\": selected_openai_model_name,\n",
        "                \"messages\": messages,\n",
        "                \"temperature\": EXPERIMENT_TEMPERATURE,  # Use configured temperature from global variable\n",
        "            }\n",
        "            completion_args['response_format'] = {\"type\": \"json_object\"}\n",
        "\n",
        "            response = openai_client.chat.completions.create(**completion_args)\n",
        "            total_tokens = response.usage.total_tokens if response.usage else 0\n",
        "            tqdm.write(f\"    └─ {criterion_name} evaluation used {total_tokens} tokens\")\n",
        "\n",
        "            response_content = response.choices[0].message.content if response.choices and response.choices[0].message else \"\"\n",
        "\n",
        "            try:\n",
        "                parsed_response = json.loads(response_content)\n",
        "                return parsed_response\n",
        "            except json.JSONDecodeError as json_e:\n",
        "                tqdm.write(f\"JSONDecodeError for {criterion_name} in proposal ID {solution_id_for_log}. Error: {json_e}\")\n",
        "                return {\n",
        "                    \"detailed_reasoning\": f\"JSON parse error: {response_content}\",\n",
        "                    \"summary_rationale\": f\"JSON parse error: {response_content[:200]}...\",\n",
        "                    \"passing_probability\": None\n",
        "                }\n",
        "\n",
        "        except (RateLimitError, APIConnectionError, APIStatusError) as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = base_delay * (2**attempt)\n",
        "                tqdm.write(f\"API error for {criterion_name}, retrying in {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "            else:\n",
        "                tqdm.write(f\"Max retries reached for {criterion_name}\")\n",
        "                return {\n",
        "                    \"detailed_reasoning\": f\"API error after retries: {e}\",\n",
        "                    \"summary_rationale\": f\"API error: {e}\",\n",
        "                    \"passing_probability\": None\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"Unexpected error for {criterion_name}: {e}\")\n",
        "            return {\n",
        "                \"detailed_reasoning\": f\"API error: {e}\",\n",
        "                \"summary_rationale\": f\"API error: {e}\",\n",
        "                \"passing_probability\": None\n",
        "            }\n",
        "\n",
        "def perform_sequential_evaluation(combined_text, prompt_form_name, example_dict=None):\n",
        "    \"\"\"\n",
        "    Perform sequential evaluation: Make 3 separate API calls, one for each criterion.\n",
        "\n",
        "    CRITICAL DESIGN: Each API call uses the EXACT SAME prompt content as simultaneous evaluation\n",
        "    (including all 3 criteria definitions AND few-shot examples if provided), but explicitly asks\n",
        "    the AI to evaluate only ONE criterion.\n",
        "\n",
        "    This design allows us to properly test for halo effect:\n",
        "    - Simultaneous: AI sees all criteria + examples, evaluates all at once → potential halo effect\n",
        "    - Sequential: AI sees all criteria + examples, but evaluates one at a time → reduced halo effect\n",
        "\n",
        "    The difference is NOT in what information the AI sees, but in what it's asked to do.\n",
        "    This is the proper way to isolate the effect of simultaneous vs sequential evaluation.\n",
        "\n",
        "    Args:\n",
        "        combined_text: Solution proposal text\n",
        "        prompt_form_name: Prompt form to use\n",
        "        example_dict: Optional dict with few-shot examples (same as simultaneous)\n",
        "\n",
        "    Returns:\n",
        "        dict: Combined results from all 3 criteria with raw response tracking\n",
        "    \"\"\"\n",
        "    solution_id_match = re.search(r'Solution ID: (\\d+)', combined_text)\n",
        "    solution_id_for_log = solution_id_match.group(1) if solution_id_match else \"N/A\"\n",
        "\n",
        "    # Log info about examples\n",
        "    examples_info = \" (with examples)\" if example_dict and any(example_dict.values()) else \" (no examples)\"\n",
        "    tqdm.write(f\"Making SEQUENTIAL evaluation for proposal ID {solution_id_for_log}{examples_info} (3 API calls, SAME prompt each time, different criterion requested)...\")\n",
        "\n",
        "    # Get ALL criteria definitions from config - SAME as simultaneous evaluation\n",
        "    config = PROMPT_CONTENT_CONFIG.get(prompt_form_name, {})\n",
        "\n",
        "    all_criteria_definitions = {\n",
        "        'potential_of_impact': config.get('criteria_impact_definition', BASE_CRITERIA_IMPACT_DEFINITION_CORE),\n",
        "        'feasibility': config.get('criteria_feasibility_definition', BASE_CRITERIA_FEASIBILITY_DEFINITION_CORE),\n",
        "        'innovative_approach': config.get('criteria_innovative_approach_definition', BASE_CRITERIA_INNOVATIVE_APPROACH_DEFINITION_CORE)\n",
        "    }\n",
        "\n",
        "\n",
        "    # Order of evaluation - evaluate all 3 criteria\n",
        "    criteria_order = ['potential_of_impact', 'feasibility', 'innovative_approach']\n",
        "\n",
        "    # ============================================================\n",
        "    # OPTIMIZATION: Parallel execution of 3 criterion API calls\n",
        "    # ============================================================\n",
        "    results = {}\n",
        "    raw_responses_dict = {}  # Use dict to maintain order later\n",
        "\n",
        "    tqdm.write(f\"  → Requesting ALL 3 criteria in parallel: {', '.join(criteria_order)}\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=3) as inner_executor:\n",
        "        # Submit all 3 API calls simultaneously\n",
        "        future_to_criterion = {\n",
        "            inner_executor.submit(\n",
        "                perform_sequential_api_call_single_criterion,\n",
        "                combined_text,\n",
        "                criterion_name,\n",
        "                prompt_form_name,\n",
        "                all_criteria_definitions,\n",
        "                example_dict\n",
        "            ): criterion_name\n",
        "            for criterion_name in criteria_order\n",
        "        }\n",
        "\n",
        "        # Collect results as they complete\n",
        "        for future in as_completed(future_to_criterion):\n",
        "            criterion_name = future_to_criterion[future]\n",
        "            try:\n",
        "                result = future.result()\n",
        "                results[criterion_name] = result\n",
        "                raw_responses_dict[criterion_name] = f\"{criterion_name}: {json.dumps(result)}\"\n",
        "            except Exception as exc:\n",
        "                tqdm.write(f\"  ✗ {criterion_name} generated an exception: {exc}\")\n",
        "                results[criterion_name] = {\n",
        "                    \"detailed_reasoning\": f\"API error: {exc}\",\n",
        "                    \"summary_rationale\": f\"API error: {exc}\",\n",
        "                    \"passing_probability\": None\n",
        "                }\n",
        "                raw_responses_dict[criterion_name] = f\"{criterion_name}: ERROR - {exc}\"\n",
        "\n",
        "    # Combine raw responses in consistent order\n",
        "    raw_responses = [raw_responses_dict.get(c, \"\") for c in criteria_order]\n",
        "    results['raw_response_content'] = \"\\n\\n\".join(raw_responses)\n",
        "\n",
        "    tqdm.write(f\"  Completed sequential evaluation for proposal ID {solution_id_for_log}\")\n",
        "    tqdm.write(f\"  Note: Each call used the SAME full prompt (all 3 criteria + examples), but requested only 1 criterion score\")\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "RAp9W1w7yyGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Evaluate each criterion separately, but with full context AND examples\n",
        "    results = {}\n",
        "    raw_responses = []\n",
        "\n",
        "    # Order of evaluation - evaluate all 3 criteria\n",
        "    criteria_order = ['potential_of_impact', 'feasibility', 'innovative_approach']\n",
        "\n",
        "    for criterion_name in criteria_order:\n",
        "        tqdm.write(f\"  → Requesting evaluation of: {criterion_name} (with full prompt context + examples)\")\n",
        "        result = perform_sequential_api_call_single_criterion(\n",
        "            combined_text,\n",
        "            criterion_name,\n",
        "            prompt_form_name,\n",
        "            all_criteria_definitions,  # Pass ALL criteria definitions, not just one\n",
        "            example_dict  # Pass examples - SAME as simultaneous\n",
        "        )\n",
        "        results[criterion_name] = result\n",
        "        raw_responses.append(f\"{criterion_name}: {json.dumps(result)}\")\n",
        "\n",
        "    # Combine results in the same format as simultaneous evaluation\n",
        "    results['raw_response_content'] = \"\\n\\n\".join(raw_responses)\n",
        "\n",
        "    tqdm.write(f\"  Completed sequential evaluation for proposal ID {solution_id_for_log}\")\n",
        "    tqdm.write(f\"  Note: Each call used the SAME full prompt (all 3 criteria + examples), but requested only 1 criterion score\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "HfKAu1ZQoQTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Orchestration for a Single Prompt Form"
      ],
      "metadata": {
        "id": "lAqLL4MrheGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_proposals_for_form(df_processed_data, form_name, example_solution_ids=None):\n",
        "    \"\"\"\n",
        "    Orchestrates the evaluation process for a single specified prompt form across all proposals.\n",
        "    Handles parallel API calls for each proposal and populates the DataFrame with results.\n",
        "\n",
        "    NOW SUPPORTS TWO EVALUATION METHODS (controlled by EXPERIMENT_EVALUATION_METHOD):\n",
        "    - 'simultaneous': All criteria in one API call (faster, potential halo effect)\n",
        "    - 'sequential': Each criterion in separate API calls (slower, reduces halo effect)\n",
        "    \"\"\"\n",
        "    output_df = df_processed_data.copy()\n",
        "\n",
        "    # Display which evaluation method is being used\n",
        "    eval_method = EXPERIMENT_EVALUATION_METHOD\n",
        "    tqdm.write(f\"\\n--- Starting Evaluation for Prompt Form: '{form_name}' ---\")\n",
        "    tqdm.write(f\"--- Evaluation Method: {eval_method.upper()} ({EVALUATION_METHODS[eval_method]}) ---\")\n",
        "    tqdm.write(f\"--- Temperature: {EXPERIMENT_TEMPERATURE} ---\\n\")\n",
        "\n",
        "    # Adjust max_workers based on evaluation method\n",
        "    # Sequential method makes 3x more API calls, so reduce parallelism to avoid rate limits\n",
        "\n",
        "    if eval_method == 'sequential':\n",
        "        max_workers = 3  # Increased since inner calls are now parallel (was 2)\n",
        "        tqdm.write(f\"Note: Using max_workers={max_workers} for sequential evaluation (3 PARALLEL API calls per proposal)\")\n",
        "    else:\n",
        "        max_workers = 5  # Original value for simultaneous (1 call per proposal)\n",
        "\n",
        "    # Prepare dynamic example texts if IDs are provided AND the form is configured to use them\n",
        "    current_form_config = PROMPT_CONTENT_CONFIG.get(form_name)\n",
        "    dynamic_example_dict_for_prompt = {}\n",
        "    if current_form_config and current_form_config.get(\"use_dynamic_few_shot_examples\") and example_solution_ids:\n",
        "        for example_type, solution_ids in example_solution_ids.items():\n",
        "            dynamic_example_dict_for_prompt[example_type] = []\n",
        "            for sol_id in solution_ids:\n",
        "                # Ensure sol_id is treated as integer for comparison\n",
        "                if sol_id in df_processed_data[\"Solution ID\"].values:\n",
        "                    # Retrieve the full combined text for the example solution\n",
        "                    solution_text = df_processed_data[df_processed_data[\"Solution ID\"] == sol_id].iloc[0][\"Combined\"]\n",
        "                    dynamic_example_dict_for_prompt[example_type].append((sol_id, solution_text))\n",
        "                else:\n",
        "                    tqdm.write(f\"Warning: Example Solution ID {sol_id} not found in the dataframe for '{example_type}'.\")\n",
        "\n",
        "\n",
        "    futures_list = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        for index, row in tqdm(output_df.iterrows(), total=len(output_df), desc=f\"Submitting tasks for {form_name}\", unit=\"tasks\", unit_scale=True, ncols=100, ascii=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"):\n",
        "            combined_text = row['Combined']\n",
        "\n",
        "            # Choose evaluation method based on EXPERIMENT_EVALUATION_METHOD\n",
        "            if eval_method == 'simultaneous':\n",
        "                # Original method: single API call for all criteria\n",
        "                future_evaluation = executor.submit(\n",
        "                    perform_openai_api_call,\n",
        "                    combined_text,\n",
        "                    form_name,\n",
        "                    example_dict=dynamic_example_dict_for_prompt\n",
        "                )\n",
        "            else:  # sequential\n",
        "                # New method: separate API calls for each criterion\n",
        "                # IMPORTANT: Pass the SAME example_dict as simultaneous\n",
        "                future_evaluation = executor.submit(\n",
        "                    perform_sequential_evaluation,\n",
        "                    combined_text,\n",
        "                    form_name,\n",
        "                    example_dict=dynamic_example_dict_for_prompt  # Pass examples - SAME as simultaneous\n",
        "                )\n",
        "\n",
        "            futures_list.append((index, future_evaluation, combined_text)) # Simplified item structure\n",
        "\n",
        "        results_from_api_calls = []\n",
        "        # Use tqdm for collecting results as well\n",
        "        for item in tqdm(futures_list, desc=f\"Collecting results for {form_name}\", unit=\"results\", unit_scale=True, ncols=100, ascii=True, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]\"):\n",
        "            index = item[0]\n",
        "            row_results = {\n",
        "                'original_index': index,\n",
        "                'Combined': item[2] # Pass combined_text through\n",
        "            }\n",
        "\n",
        "            future_evaluation = item[1]\n",
        "            try:\n",
        "                unified_result = future_evaluation.result()\n",
        "                row_results['feasibility'] = unified_result.get('feasibility', {})\n",
        "                row_results['potential_of_impact'] = unified_result.get('potential_of_impact', {})\n",
        "                row_results['innovative_approach'] = unified_result.get('innovative_approach', {})\n",
        "                row_results['raw_response_content_unified'] = unified_result.get('raw_response_content')\n",
        "            except Exception as exc:\n",
        "                tqdm.write(f\"Task for row {index} generated an exception: {exc}\")\n",
        "                row_results['feasibility'] = {\"detailed_reasoning\": f\"API error: {exc}\", \"summary_rationale\": f\"API error: {exc}\", \"passing_probability\": None}\n",
        "                row_results['potential_of_impact'] = {\"detailed_reasoning\": f\"API error: {exc}\", \"summary_rationale\": f\"API error: {exc}\", \"passing_probability\": None}\n",
        "                row_results['innovative_approach'] = {\"detailed_reasoning\": f\"API error: {exc}\", \"summary_rationale\": f\"API error: {exc}\", \"passing_probability\": None}\n",
        "                row_results['raw_response_content_unified'] = f\"API call failed: {exc}\"\n",
        "\n",
        "            results_from_api_calls.append(row_results)\n",
        "\n",
        "    tqdm.write(f\"API calls complete for '{form_name}'. Populating DataFrame...\")\n",
        "\n",
        "    # Initialize all columns that will be populated to avoid KeyError\n",
        "    output_df['feasibility_detailed_reasoning'] = None\n",
        "    output_df['feasibility_summary_rationale'] = None\n",
        "    output_df['feasibility_passing_probability'] = None\n",
        "    output_df['potential_of_impact_detailed_reasoning'] = None\n",
        "    output_df['potential_of_impact_summary_rationale'] = None\n",
        "    output_df['potential_of_impact_passing_probability'] = None\n",
        "    output_df['innovative_approach_detailed_reasoning'] = None\n",
        "    output_df['innovative_approach_summary_rationale'] = None\n",
        "    output_df['innovative_approach_passing_probability'] = None\n",
        "    output_df['unified_raw_response_content'] = None\n",
        "\n",
        "\n",
        "    for r in results_from_api_calls:\n",
        "        idx = r['original_index']\n",
        "\n",
        "        # Populate Feasibility details\n",
        "        output_df.loc[idx, 'feasibility_detailed_reasoning'] = r['feasibility'].get('detailed_reasoning')\n",
        "        output_df.loc[idx, 'feasibility_summary_rationale'] = r['feasibility'].get('summary_rationale')\n",
        "        output_df.loc[idx, 'feasibility_passing_probability'] = r['feasibility'].get('passing_probability')\n",
        "\n",
        "        # Populate Impact details\n",
        "        output_df.loc[idx, 'potential_of_impact_detailed_reasoning'] = r['potential_of_impact'].get('detailed_reasoning')\n",
        "        output_df.loc[idx, 'potential_of_impact_summary_rationale'] = r['potential_of_impact'].get('summary_rationale')\n",
        "        output_df.loc[idx, 'potential_of_impact_passing_probability'] = r['potential_of_impact'].get('passing_probability')\n",
        "\n",
        "        # Populate Innovative Approach details\n",
        "        innovative_approach_data = r.get('innovative_approach', {})\n",
        "        output_df.loc[idx, 'innovative_approach_detailed_reasoning'] = innovative_approach_data.get('detailed_reasoning')\n",
        "        output_df.loc[idx, 'innovative_approach_summary_rationale'] = innovative_approach_data.get('summary_rationale')\n",
        "        output_df.loc[idx, 'innovative_approach_passing_probability'] = innovative_approach_data.get('passing_probability')\n",
        "\n",
        "        # Raw response content (always unified now)\n",
        "        output_df.loc[idx, 'unified_raw_response_content'] = r.get('raw_response_content_unified')\n",
        "\n",
        "\n",
        "    # Local Overall Assessment (synthesis from individual summaries)\n",
        "    output_df['overall_holistic_rationale'] = output_df.apply(\n",
        "        lambda row: f\"Feasibility Summary: {row.get('feasibility_summary_rationale', 'N/A')}\\n\"\n",
        "                    f\"Impact Summary: {row.get('potential_of_impact_summary_rationale', 'N/A')}\\n\"\n",
        "                    f\"Innovative Approach Summary: {row.get('innovative_approach_summary_rationale', 'N/A')}\",\n",
        "        axis=1\n",
        "    )\n",
        "    output_df['overall_passing_probability'] = None # Initialized, will be overwritten by weighted score\n",
        "\n",
        "    return output_df\n"
      ],
      "metadata": {
        "id": "CYxy3Tuehd9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    if eval_method == 'sequential':\n",
        "        max_workers = 2  # Reduced for sequential (3 calls per proposal)\n",
        "        tqdm.write(f\"Note: Using max_workers={max_workers} for sequential evaluation (3 API calls per proposal)\")\n",
        "    else:\n",
        "        max_workers = 5  # Original value for simultaneous (1 call per proposal)\n"
      ],
      "metadata": {
        "id": "7H1yAhgMp2sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculations and Plotting"
      ],
      "metadata": {
        "id": "PcODmy8RhjO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_scores_to_numeric(df_input):\n",
        "    \"\"\"\n",
        "    Converts passing probability columns to numeric format.\n",
        "    \"\"\"\n",
        "    df = df_input.copy() # Work on a copy to avoid modifying original outside this function\n",
        "\n",
        "    # Convert passing probability columns to numeric, coercing errors to NaN\n",
        "    df['feasibility_passing_probability_numeric'] = pd.to_numeric(df['feasibility_passing_probability'], errors='coerce')\n",
        "    df['potential_of_impact_passing_probability_numeric'] = pd.to_numeric(df['potential_of_impact_passing_probability'], errors='coerce')\n",
        "    df['innovative_approach_passing_probability_numeric'] = pd.to_numeric(df['innovative_approach_passing_probability'], errors='coerce')\n",
        "\n",
        "    return df\n",
        "\n",
        "def generate_heatmaps_logic(df_input, form_name):\n",
        "    \"\"\"\n",
        "    Generates and saves two heatmaps:\n",
        "    1. Proposal scores across all criteria (with Solution ID on Y-axis).\n",
        "    2. Correlation matrix between criteria passing probabilities.\n",
        "    \"\"\"\n",
        "    df = df_input.copy() # Work on a copy\n",
        "\n",
        "    # Plot 1: Heatmap showing each proposal's scores across criteria\n",
        "    tqdm.write(f\"\\nGenerating Proposals vs. Criteria Scores Heatmap for '{form_name}'...\")\n",
        "    # Define columns to be included in the heatmap, including 'Solution ID'\n",
        "    heatmap_data_columns = [\n",
        "        'Solution ID', # Ensure Solution ID is included to be set as index\n",
        "        'feasibility_passing_probability_numeric',\n",
        "        'potential_of_impact_passing_probability_numeric',\n",
        "        'innovative_approach_passing_probability_numeric'\n",
        "    ]\n",
        "    # Filter for columns that actually exist in the DataFrame to prevent KeyError\n",
        "    actual_heatmap_cols = [col for col in heatmap_data_columns if col in df.columns]\n",
        "\n",
        "    # Select the columns for plotting. Create a copy to avoid SettingWithCopyWarning.\n",
        "    plot_scores_df = df[actual_heatmap_cols].copy()\n",
        "\n",
        "    if not plot_scores_df.empty:\n",
        "        # Set 'Solution ID' as the index for the DataFrame, which will then be used as the Y-axis labels\n",
        "        if 'Solution ID' in plot_scores_df.columns:\n",
        "            # Drop rows where any of the numeric score columns are NaN *before* setting index\n",
        "            # This ensures that all data in the heatmap is valid numeric and has a corresponding Solution ID.\n",
        "            numeric_score_cols_for_plot = [col for col in plot_scores_df.columns if col != 'Solution ID']\n",
        "            plot_scores_df = plot_scores_df.dropna(subset=numeric_score_cols_for_plot)\n",
        "\n",
        "            # Check again if plot_scores_df is empty AFTER dropna, as it might become empty\n",
        "            if plot_scores_df.empty:\n",
        "                tqdm.write(f\"Not enough valid numeric scores after cleaning to generate Proposal Scores Heatmap for '{form_name}'. Skipping heatmap.\")\n",
        "                return # Exit function if no data to plot\n",
        "\n",
        "            # Now set the index\n",
        "            # Ensure Solution ID is of a type suitable for indexing (e.g., string or int) and unique\n",
        "            plot_scores_df['Solution ID'] = plot_scores_df['Solution ID'].astype(str)\n",
        "            plot_scores_df = plot_scores_df.set_index('Solution ID')\n",
        "\n",
        "        # Rename the remaining columns for better readability in the plot\n",
        "        plot_scores_df.columns = ['Feasibility', 'Impact', 'Innovative Approach']\n",
        "\n",
        "        plt.figure(figsize=(12, max(6, len(plot_scores_df) * 0.4))) # Dynamic height, with a minimum of 6 inches\n",
        "        # Create heatmap\n",
        "        sns.heatmap(plot_scores_df, annot=True, cmap='viridis', fmt=\".0f\", linewidths=.5, vmin=1, vmax=5) # Set vmin/vmax for 5-point scale\n",
        "        plt.title(f'Proposal Scores Across Criteria and Total Score (Form: {form_name})')\n",
        "        plt.xlabel('Evaluation Criteria')\n",
        "        plt.ylabel('Proposal Solution ID') # Update Y-axis label to reflect Solution ID\n",
        "        plt.tight_layout() # Adjust plot to prevent labels overlapping\n",
        "        # Save and show plot\n",
        "        plt.savefig(f'proposal_criteria_scores_heatmap_{form_name}.png')\n",
        "        plt.show() # Display plot (for interactive environments like Colab)\n",
        "        tqdm.write(f\"Proposal scores heatmap for '{form_name}' saved as 'proposal_criteria_scores_heatmap_{form_name}.png'.\")\n",
        "    else:\n",
        "        tqdm.write(f\"Not enough valid numeric scores or Solution IDs to generate Proposal Scores Heatmap for '{form_name}'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1fxYC9h3hjFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Execution"
      ],
      "metadata": {
        "id": "9AcklJokhskW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This section orchestrates the entire evaluation workflow.\n",
        "\n",
        "# Load and preprocess data once at the top level\n",
        "df_processed_data = load_and_preprocess_data()"
      ],
      "metadata": {
        "id": "m4B1d3Tni7P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example usage with dynamic few-shot example for score"
      ],
      "metadata": {
        "id": "K4hSWZz47X21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage with dynamic few-shot examples using 5-point scoring scale\n",
        "# These IDs must exist in your loaded health.xlsx or dummy data.\n",
        "# For 3 criteria: Potential for Impact (criteria1), Feasibility (criteria2), Innovative Approach (criteria3)\n",
        "\n",
        "CUSTOM_FEW_SHOT_EXAMPLES = {\n",
        "    'high_scores': [97613],      # Example Solution ID for high scores (5) across all criteria\n",
        "    'medium_scores': [97530],    # Example Solution ID for medium scores (3) across criteria\n",
        "    'low_scores': [97882],       # NEW: Example Solution ID for overall low scores (2) across multiple criteria\n",
        "    'criteria1_low_score': [96399],  # Example Solution ID with low score (1-2) on Potential for Impact\n",
        "    'criteria2_low_score': [95014],  # Example Solution ID with low score (1-2) on Feasibility\n",
        "    'criteria3_low_score': [95356]   # Example Solution ID with low score (1-2) on Innovative Approach\n",
        "}"
      ],
      "metadata": {
        "id": "oaoN4M4-7Xm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- USER SELECTION: CHOOSE YOUR PROMPT FORM HERE ---\n",
        "# Select the single prompt form to execute for this run.\n",
        "# \"default\", \"no_harsh\", \"no_rubric\", \"no_examples\", \"without_everything\"\n",
        "selected_prompt_form_name = \"without_everything\" # <--- CHANGE THIS VALUE TO SELECT A SINGLE PROMPT FORM FOR EXECUTION ### USER TO CUSTOMIZE ###\n",
        "\n",
        "print(f\"\\n===== Starting Full Evaluation for Selected Prompt Form: '{selected_prompt_form_name}' =====\")\n",
        "\n",
        "# Determine if dynamic examples should be passed to the evaluation function based on the selected form's configuration\n",
        "current_form_config = PROMPT_CONTENT_CONFIG.get(selected_prompt_form_name)\n",
        "examples_to_pass = None\n",
        "if current_form_config and current_form_config.get(\"use_dynamic_few_shot_examples\"):\n",
        "    examples_to_pass = CUSTOM_FEW_SHOT_EXAMPLES\n",
        "\n",
        "\n",
        "# 1. Execute the core evaluation process for the selected prompt form\n",
        "# This function handles API calls, results collection, and initial DataFrame population.\n",
        "final_df = evaluate_proposals_for_form(df_processed_data.copy(), selected_prompt_form_name, example_solution_ids=examples_to_pass)\n",
        "\n",
        "# 2. Perform post-processing: convert scores to numeric format\n",
        "final_df = convert_scores_to_numeric(final_df)\n",
        "\n",
        "# 3. Display a snapshot of the final evaluation results\n",
        "print(f\"\\n--- Final Evaluation Results for '{selected_prompt_form_name}' (First 3 rows for key columns) ---\")\n",
        "\n",
        "# Raw response column is always unified now\n",
        "raw_response_cols_to_display = ['unified_raw_response_content']\n",
        "\n",
        "display_columns = ['Solution ID', 'Combined',\n",
        "                   'potential_of_impact_passing_probability',\n",
        "                   'feasibility_passing_probability',\n",
        "                   'innovative_approach_passing_probability',\n",
        "                   'overall_holistic_rationale'] + raw_response_cols_to_display\n",
        "\n",
        "print(final_df[display_columns].head(3))\n",
        "\n",
        "# 4. Save the evaluation results to CSV file (not Excel) with experiment configuration in filename\n",
        "\n",
        "# Build filename with experiment parameters for easy identification\n",
        "config = get_experiment_config()\n",
        "output_csv = (f\"startup_evaluations_results_\"\n",
        "              f\"{selected_prompt_form_name}_\"\n",
        "              f\"temp{config['temperature']}_\"\n",
        "              f\"{config['evaluation_method']}.csv\")\n",
        "\n",
        "# Add experiment configuration columns to DataFrame for reference\n",
        "final_df['experiment_temperature'] = config['temperature']\n",
        "final_df['experiment_evaluation_method'] = config['evaluation_method']\n",
        "\n",
        "final_df.to_csv(output_csv, index=False)\n",
        "print(f\"\\nResults for '{selected_prompt_form_name}' saved to {output_csv}\")\n",
        "print(f\"  Temperature: {config['temperature']}\")\n",
        "print(f\"  Method: {config['evaluation_method']}\")\n",
        "\n",
        "# 5. Generate and save visualization plots (heatmaps) with experiment config in filename\n",
        "# Update heatmap filename to include experiment parameters\n",
        "heatmap_filename_suffix = f\"{selected_prompt_form_name}_temp{config['temperature']}_{config['evaluation_method']}\"\n",
        "generate_heatmaps_logic(final_df, heatmap_filename_suffix)\n",
        "\n",
        "print(f\"\\nEvaluation for '{selected_prompt_form_name}' complete. Review generated files and plots.\")\n",
        "\n",
        "# --- Optional: Download all generated files in Colab environment ---\n",
        "# This block allows automatic download of generated files if running in Google Colab.\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print(\"\\nInitiating downloads for generated files...\")\n",
        "    config = get_experiment_config()\n",
        "    heatmap_suffix = f\"{selected_prompt_form_name}_temp{config['temperature']}_{config['evaluation_method']}\"\n",
        "\n",
        "    files_to_download = [\n",
        "        f\"startup_evaluations_results_{selected_prompt_form_name}_temp{config['temperature']}_{config['evaluation_method']}.csv\",\n",
        "        f'proposal_criteria_scores_heatmap_{heatmap_suffix}.png',\n",
        "    ]\n",
        "\n",
        "    for file_path in files_to_download:\n",
        "        if os.path.exists(file_path):\n",
        "            files.download(file_path)\n",
        "    print(\"Downloads initiated. Check your downloads folder.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "mLvIHI3qUtWO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JrcMF2JVhLo1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}